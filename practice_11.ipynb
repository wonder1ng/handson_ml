{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방지 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.putenv('TF_GPU_ALLOCATOR', 'cuda_malloc_async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(10, kernel_initializer='he_normal')\n",
    "keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,246\n",
      "Trainable params: 268,878\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(momentum=0.99), # momentum: 기본 0.99 미니배치가 작을수록 소수점 뒤에 9를 넣어 1에 가깝게 만듦\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0, clipnorm=1.0)\n",
    "# clipvalue=1.0: loss의 모든 편미분 값을 -1.0 ~ 1.0으로 잘라냄.\n",
    "# clipnorm=1.0: 해당 값 기준으로 정규화\n",
    "# 두 인자 모두 기입 시 norm을 먼저 적용\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 4s 2ms/step - loss: 0.5513 - accuracy: 0.8208 - val_loss: 0.3726 - val_accuracy: 0.8762\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3520 - accuracy: 0.8778 - val_loss: 0.3200 - val_accuracy: 0.8951\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.3168 - accuracy: 0.8902 - val_loss: 0.3010 - val_accuracy: 0.8981\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2973 - accuracy: 0.8963 - val_loss: 0.2878 - val_accuracy: 0.9018\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2849 - accuracy: 0.9020 - val_loss: 0.2858 - val_accuracy: 0.9038\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2746 - accuracy: 0.9057 - val_loss: 0.2731 - val_accuracy: 0.9086\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2670 - accuracy: 0.9088 - val_loss: 0.2668 - val_accuracy: 0.9093\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2603 - accuracy: 0.9110 - val_loss: 0.2587 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2547 - accuracy: 0.9133 - val_loss: 0.2545 - val_accuracy: 0.9158\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2495 - accuracy: 0.9146 - val_loss: 0.2515 - val_accuracy: 0.9175\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2454 - accuracy: 0.9156 - val_loss: 0.2506 - val_accuracy: 0.9168\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2409 - accuracy: 0.9180 - val_loss: 0.2483 - val_accuracy: 0.9163\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2374 - accuracy: 0.9190 - val_loss: 0.2452 - val_accuracy: 0.9178\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2344 - accuracy: 0.9210 - val_loss: 0.2416 - val_accuracy: 0.9183\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2311 - accuracy: 0.9215 - val_loss: 0.2439 - val_accuracy: 0.9193\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2286 - accuracy: 0.9225 - val_loss: 0.2450 - val_accuracy: 0.9170\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2255 - accuracy: 0.9232 - val_loss: 0.2384 - val_accuracy: 0.9183\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2230 - accuracy: 0.9239 - val_loss: 0.2368 - val_accuracy: 0.9200\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2207 - accuracy: 0.9241 - val_loss: 0.2339 - val_accuracy: 0.9203\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2184 - accuracy: 0.9256 - val_loss: 0.2332 - val_accuracy: 0.9195\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.5673 - accuracy: 0.7100 - val_loss: 0.3819 - val_accuracy: 0.8357\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.3726 - accuracy: 0.8700 - val_loss: 0.2988 - val_accuracy: 0.8915\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2933 - accuracy: 0.8950 - val_loss: 0.2457 - val_accuracy: 0.9260\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2402 - accuracy: 0.9350 - val_loss: 0.2093 - val_accuracy: 0.9503\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2030 - accuracy: 0.9550 - val_loss: 0.1836 - val_accuracy: 0.9645\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.1766 - accuracy: 0.9650 - val_loss: 0.1640 - val_accuracy: 0.9757\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1561 - accuracy: 0.9750 - val_loss: 0.1496 - val_accuracy: 0.9767\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1408 - accuracy: 0.9800 - val_loss: 0.1373 - val_accuracy: 0.9787\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9900 - val_loss: 0.1283 - val_accuracy: 0.9797\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1173 - accuracy: 0.9900 - val_loss: 0.1202 - val_accuracy: 0.9817\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1085 - accuracy: 0.9900 - val_loss: 0.1133 - val_accuracy: 0.9828\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1011 - accuracy: 0.9900 - val_loss: 0.1067 - val_accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0936 - accuracy: 0.9950 - val_loss: 0.1011 - val_accuracy: 0.9828\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0880 - accuracy: 0.9950 - val_loss: 0.0968 - val_accuracy: 0.9828\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0829 - accuracy: 0.9950 - val_loss: 0.0928 - val_accuracy: 0.9838\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0782 - accuracy: 0.9950 - val_loss: 0.0893 - val_accuracy: 0.9848\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0744 - accuracy: 0.9950 - val_loss: 0.0859 - val_accuracy: 0.9858\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0704 - accuracy: 0.9950 - val_loss: 0.0831 - val_accuracy: 0.9858\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0670 - accuracy: 0.9950 - val_loss: 0.0806 - val_accuracy: 0.9858\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0640 - accuracy: 0.9950 - val_loss: 0.0781 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # output 제외 전체 layer 반환\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)   # 모델 구조 복사, 가중치는 복제하지 않음\n",
    "model_A_clone.set_weights(model_A.get_weights())    # 가중치 복제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False # 출력층 제외 가중치 동결\n",
    "# 층을 동결하거나 동결 해제 후 새로 컴파일 필수\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.7557 - accuracy: 0.5550 - val_loss: 0.5122 - val_accuracy: 0.7272\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3968 - accuracy: 0.8100 - val_loss: 0.3248 - val_accuracy: 0.8692\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2588 - accuracy: 0.9050 - val_loss: 0.2393 - val_accuracy: 0.9219\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1928 - accuracy: 0.9500 - val_loss: 0.1959 - val_accuracy: 0.9432\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 38ms/step - loss: 0.1675 - accuracy: 0.9700 - val_loss: 0.1927 - val_accuracy: 0.9442\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.1643 - accuracy: 0.9700 - val_loss: 0.1893 - val_accuracy: 0.9473\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1611 - accuracy: 0.9700 - val_loss: 0.1863 - val_accuracy: 0.9473\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1583 - accuracy: 0.9700 - val_loss: 0.1833 - val_accuracy: 0.9503\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1554 - accuracy: 0.9700 - val_loss: 0.1802 - val_accuracy: 0.9513\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1524 - accuracy: 0.9700 - val_loss: 0.1775 - val_accuracy: 0.9523\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1499 - accuracy: 0.9700 - val_loss: 0.1748 - val_accuracy: 0.9533\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1473 - accuracy: 0.9700 - val_loss: 0.1722 - val_accuracy: 0.9533\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1447 - accuracy: 0.9700 - val_loss: 0.1698 - val_accuracy: 0.9533\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1425 - accuracy: 0.9750 - val_loss: 0.1675 - val_accuracy: 0.9544\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1403 - accuracy: 0.9750 - val_loss: 0.1651 - val_accuracy: 0.9564\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.1380 - accuracy: 0.9800 - val_loss: 0.1628 - val_accuracy: 0.9564\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1357 - accuracy: 0.9800 - val_loss: 0.1607 - val_accuracy: 0.9574\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1336 - accuracy: 0.9800 - val_loss: 0.1585 - val_accuracy: 0.9574\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1314 - accuracy: 0.9850 - val_loss: 0.1562 - val_accuracy: 0.9574\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.1293 - accuracy: 0.9850 - val_loss: 0.1542 - val_accuracy: 0.9574\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)    # 전이 학습은 학습률을 더 낮게 줌\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1526 - accuracy: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15260308980941772, 0.9635000228881836]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "54/54 [==============================] - 1s 7ms/step - loss: 27.6201 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6197 - lr: 0.0089\n",
      "Epoch 3/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6194 - lr: 0.0079\n",
      "Epoch 4/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6192 - lr: 0.0071\n",
      "Epoch 5/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6190 - lr: 0.0063\n",
      "Epoch 6/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6189 - lr: 0.0056\n",
      "Epoch 7/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6187 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6187 - lr: 0.0045\n",
      "Epoch 9/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6186 - lr: 0.0040\n",
      "Epoch 10/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6185 - lr: 0.0035\n",
      "Epoch 11/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6184 - lr: 0.0032\n",
      "Epoch 12/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6184 - lr: 0.0028\n",
      "Epoch 13/200\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 27.6183 - lr: 0.0025\n",
      "Epoch 14/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6183 - lr: 0.0022\n",
      "Epoch 15/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6183 - lr: 0.0020\n",
      "Epoch 16/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6182 - lr: 0.0018\n",
      "Epoch 17/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6182 - lr: 0.0016\n",
      "Epoch 18/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6182 - lr: 0.0014\n",
      "Epoch 19/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6182 - lr: 0.0013\n",
      "Epoch 20/200\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 27.6181 - lr: 0.0011\n",
      "Epoch 21/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 8.9125e-04\n",
      "Epoch 23/200\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 27.6181 - lr: 7.9433e-04\n",
      "Epoch 24/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 7.0795e-04\n",
      "Epoch 25/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 6.3096e-04\n",
      "Epoch 26/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 5.6234e-04\n",
      "Epoch 27/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6181 - lr: 5.0119e-04\n",
      "Epoch 28/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 4.4668e-04\n",
      "Epoch 29/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.9811e-04\n",
      "Epoch 30/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.5481e-04\n",
      "Epoch 31/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.1623e-04\n",
      "Epoch 32/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.8184e-04\n",
      "Epoch 33/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.5119e-04\n",
      "Epoch 34/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.2387e-04\n",
      "Epoch 35/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-04\n",
      "Epoch 36/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.7783e-04\n",
      "Epoch 37/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.5849e-04\n",
      "Epoch 38/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.4125e-04\n",
      "Epoch 39/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.2589e-04\n",
      "Epoch 40/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.1220e-04\n",
      "Epoch 41/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-05\n",
      "Epoch 43/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.9433e-05\n",
      "Epoch 44/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.0795e-05\n",
      "Epoch 45/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 6.3096e-05\n",
      "Epoch 46/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 5.6234e-05\n",
      "Epoch 47/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 5.0119e-05\n",
      "Epoch 48/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 4.4668e-05\n",
      "Epoch 49/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.9811e-05\n",
      "Epoch 50/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.5481e-05\n",
      "Epoch 51/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.1623e-05\n",
      "Epoch 52/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.8184e-05\n",
      "Epoch 53/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.5119e-05\n",
      "Epoch 54/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.2387e-05\n",
      "Epoch 55/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-05\n",
      "Epoch 56/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.7783e-05\n",
      "Epoch 57/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.5849e-05\n",
      "Epoch 58/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.4125e-05\n",
      "Epoch 59/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.2589e-05\n",
      "Epoch 60/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.1220e-05\n",
      "Epoch 61/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.0000e-05\n",
      "Epoch 62/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-06\n",
      "Epoch 63/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.9433e-06\n",
      "Epoch 64/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 7.0795e-06\n",
      "Epoch 65/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 6.3096e-06\n",
      "Epoch 66/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-06\n",
      "Epoch 67/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 5.0119e-06\n",
      "Epoch 68/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 4.4668e-06\n",
      "Epoch 69/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.9811e-06\n",
      "Epoch 70/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.5481e-06\n",
      "Epoch 71/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.1623e-06\n",
      "Epoch 72/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.8184e-06\n",
      "Epoch 73/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.5119e-06\n",
      "Epoch 74/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.2387e-06\n",
      "Epoch 75/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.9953e-06\n",
      "Epoch 76/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.7783e-06\n",
      "Epoch 77/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.5849e-06\n",
      "Epoch 78/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.4125e-06\n",
      "Epoch 79/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.2589e-06\n",
      "Epoch 80/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.1220e-06\n",
      "Epoch 81/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.0000e-06\n",
      "Epoch 82/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 8.9125e-07\n",
      "Epoch 83/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.9433e-07\n",
      "Epoch 84/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.0795e-07\n",
      "Epoch 85/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 6.3096e-07\n",
      "Epoch 86/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 5.6234e-07\n",
      "Epoch 87/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 5.0119e-07\n",
      "Epoch 88/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 4.4668e-07\n",
      "Epoch 89/200\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 27.6180 - lr: 3.9811e-07\n",
      "Epoch 90/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 3.5481e-07\n",
      "Epoch 91/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 3.1623e-07\n",
      "Epoch 92/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.8184e-07\n",
      "Epoch 93/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.5119e-07\n",
      "Epoch 94/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.2387e-07\n",
      "Epoch 95/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-07\n",
      "Epoch 96/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.7783e-07\n",
      "Epoch 97/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.5849e-07\n",
      "Epoch 98/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.4125e-07\n",
      "Epoch 99/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.2589e-07\n",
      "Epoch 100/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.1220e-07\n",
      "Epoch 101/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.0000e-07\n",
      "Epoch 102/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-08\n",
      "Epoch 103/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.9433e-08\n",
      "Epoch 104/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.0795e-08\n",
      "Epoch 105/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 6.3096e-08\n",
      "Epoch 106/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-08\n",
      "Epoch 107/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 5.0119e-08\n",
      "Epoch 108/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 4.4668e-08\n",
      "Epoch 109/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 3.9811e-08\n",
      "Epoch 110/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.5481e-08\n",
      "Epoch 111/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 3.1623e-08\n",
      "Epoch 112/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.8184e-08\n",
      "Epoch 113/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.5119e-08\n",
      "Epoch 114/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.2387e-08\n",
      "Epoch 115/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-08\n",
      "Epoch 116/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.7783e-08\n",
      "Epoch 117/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.5849e-08\n",
      "Epoch 118/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.4125e-08\n",
      "Epoch 119/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.2589e-08\n",
      "Epoch 120/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.1220e-08\n",
      "Epoch 121/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.0000e-08\n",
      "Epoch 122/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 8.9125e-09\n",
      "Epoch 123/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.9433e-09\n",
      "Epoch 124/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.0795e-09\n",
      "Epoch 125/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 6.3096e-09\n",
      "Epoch 126/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-09\n",
      "Epoch 127/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.0119e-09\n",
      "Epoch 128/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 4.4668e-09\n",
      "Epoch 129/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 3.9811e-09\n",
      "Epoch 130/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.5481e-09\n",
      "Epoch 131/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.1623e-09\n",
      "Epoch 132/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.8184e-09\n",
      "Epoch 133/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.5119e-09\n",
      "Epoch 134/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 2.2387e-09\n",
      "Epoch 135/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.9953e-09\n",
      "Epoch 136/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.7783e-09\n",
      "Epoch 137/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 1.5849e-09\n",
      "Epoch 138/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.4125e-09\n",
      "Epoch 139/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.2589e-09\n",
      "Epoch 140/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.1220e-09\n",
      "Epoch 141/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.0000e-09\n",
      "Epoch 142/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-10\n",
      "Epoch 143/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.9433e-10\n",
      "Epoch 144/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.0795e-10\n",
      "Epoch 145/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 6.3096e-10\n",
      "Epoch 146/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-10\n",
      "Epoch 147/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.0119e-10\n",
      "Epoch 148/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 4.4668e-10\n",
      "Epoch 149/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.9811e-10\n",
      "Epoch 150/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.5481e-10\n",
      "Epoch 151/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.1623e-10\n",
      "Epoch 152/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.8184e-10\n",
      "Epoch 153/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 2.5119e-10\n",
      "Epoch 154/200\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 27.6180 - lr: 2.2387e-10\n",
      "Epoch 155/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-10\n",
      "Epoch 156/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.7783e-10\n",
      "Epoch 157/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.5849e-10\n",
      "Epoch 158/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.4125e-10\n",
      "Epoch 159/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.2589e-10\n",
      "Epoch 160/200\n",
      "54/54 [==============================] - 1s 12ms/step - loss: 27.6180 - lr: 1.1220e-10\n",
      "Epoch 161/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.0000e-10\n",
      "Epoch 162/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-11\n",
      "Epoch 163/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.9433e-11\n",
      "Epoch 164/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 7.0795e-11\n",
      "Epoch 165/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 6.3096e-11\n",
      "Epoch 166/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-11\n",
      "Epoch 167/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.0119e-11\n",
      "Epoch 168/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 4.4668e-11\n",
      "Epoch 169/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.9811e-11\n",
      "Epoch 170/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.5481e-11\n",
      "Epoch 171/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 3.1623e-11\n",
      "Epoch 172/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.8184e-11\n",
      "Epoch 173/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.5119e-11\n",
      "Epoch 174/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 2.2387e-11\n",
      "Epoch 175/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-11\n",
      "Epoch 176/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.7783e-11\n",
      "Epoch 177/200\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 27.6180 - lr: 1.5849e-11\n",
      "Epoch 178/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.4125e-11\n",
      "Epoch 179/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.2589e-11\n",
      "Epoch 180/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.1220e-11\n",
      "Epoch 181/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.0000e-11\n",
      "Epoch 182/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 8.9125e-12\n",
      "Epoch 183/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.9433e-12\n",
      "Epoch 184/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 7.0795e-12\n",
      "Epoch 185/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 6.3096e-12\n",
      "Epoch 186/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.6234e-12\n",
      "Epoch 187/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 5.0119e-12\n",
      "Epoch 188/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 4.4668e-12\n",
      "Epoch 189/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 3.9811e-12\n",
      "Epoch 190/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 3.5481e-12\n",
      "Epoch 191/200\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 27.6180 - lr: 3.1623e-12\n",
      "Epoch 192/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 2.8184e-12\n",
      "Epoch 193/200\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 27.6180 - lr: 2.5119e-12\n",
      "Epoch 194/200\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 27.6180 - lr: 2.2387e-12\n",
      "Epoch 195/200\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 27.6180 - lr: 1.9953e-12\n",
      "Epoch 196/200\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 27.6180 - lr: 1.7783e-12\n",
      "Epoch 197/200\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 27.6180 - lr: 1.5849e-12\n",
      "Epoch 198/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.4125e-12\n",
      "Epoch 199/200\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 27.6180 - lr: 1.2589e-12\n",
      "Epoch 200/200\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 27.6180 - lr: 1.1220e-12\n"
     ]
    }
   ],
   "source": [
    "# 거듭제곱법\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n",
    "\n",
    "# 지수\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch/20)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return 0.01 * 0.1**(epoch/20)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "# callback 기능을 이용하기 때문에 위의 형태로 작성\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train, y_train, batch_size=1028, epochs=200, callbacks=[lr_scheduler])\n",
    "\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1/20)\n",
    "\n",
    "# 구간별 고정\n",
    "def piecewuse_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch <15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# 성능 기반\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# 연속 patience에폭 동안 val_loss가 개션되지 않을 때 factor를 학습률에 곱함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "### 1사이클\n",
    "# 최적 학습률 확인\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.prev_loss = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(model.optimizer.lr.numpy())\n",
    "        self.losses.append(batch_loss)\n",
    "        self.model.optimizer.lr = self.model.optimizer.lr * self.factor\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = model.optimizer.lr.numpy()\n",
    "    model.optimizer.lr = min_rate\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    model.optimizer.lr = init_lr\n",
    "    model.set_weights(init_weights)\n",
    "\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "# 1사이클 클래스\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n",
    "        self.total_iteration = iterations  # 총 학습률 조정 반복 횟수\n",
    "        self.max_rate = max_rate  # 최대 학습률\n",
    "        self.start_rate = start_rate or max_rate / 10  # 시작 학습률 (디폴트는 최대 학습률의 10%)\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1  # 마지막 단계의 반복 횟수 (디폴트는 총 반복 횟수의 10%)\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2  # 중간 단계 반복 횟수\n",
    "        self.last_rate = last_rate or self.start_rate / 1000  # 마지막 학습률 (디폴트는 시작 학습률의 1/1000)\n",
    "        self.current_iteration = 0  # 현재 반복 횟수 초기화\n",
    "\n",
    "    def _interpolate(self, from_iter, to_iter2, from_rate, to_rate):\n",
    "        # 두 지점 사이에서 선형 보간을 통해 학습률 계산하여 to_iter까지 선형적으로 rate 변화\n",
    "        return ((to_rate - from_rate) * (self.current_iteration - from_iter) / (to_iter2 - from_iter) + from_rate)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.current_iteration < self.half_iteration:\n",
    "            # 초기 상승 단계\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.current_iteration < 2 * self.half_iteration:\n",
    "            # 최대 학습률로 상승한 후 하락 단계\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            # 마지막 하락 단계\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.total_iteration, self.start_rate, self.last_rate)\n",
    "        self.current_iteration += 1  # 반복 횟수 증가\n",
    "        self.model.optimizer.lr = rate  # 모델의 학습률 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 규제 적용 방식\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l1_l2(0.01, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function elu at 0x0000025E2C58A8B0>\n",
      "<function relu at 0x0000025E2C58E160>\n",
      "<function softmax at 0x0000025E2C58A700>\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "# partial: 함수의 인자 기본값을 새로 지정하여 사용할 수 있게 함.\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l1_l2(0.01, 0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100, activation='relu'),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])\n",
    "\n",
    "[print(layer.activation) for layer in model.layers[1:]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[29, 29]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 2s 15ms/step - loss: 2.3304 - accuracy: 0.1405 - val_loss: 2.1615 - val_accuracy: 0.2270\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 2.1521 - accuracy: 0.1932 - val_loss: 2.0471 - val_accuracy: 0.2680\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 2.0768 - accuracy: 0.2213 - val_loss: 2.0022 - val_accuracy: 0.2609\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 2.0378 - accuracy: 0.2386 - val_loss: 1.9380 - val_accuracy: 0.3105\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 2.0107 - accuracy: 0.2500 - val_loss: 1.9274 - val_accuracy: 0.2968\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 1.9882 - accuracy: 0.2582 - val_loss: 1.9090 - val_accuracy: 0.3168\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 1.9856 - accuracy: 0.2625 - val_loss: 1.8932 - val_accuracy: 0.3341\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 1.9697 - accuracy: 0.2697 - val_loss: 1.8712 - val_accuracy: 0.3403\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9577 - accuracy: 0.2778 - val_loss: 1.8900 - val_accuracy: 0.3307\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9717 - accuracy: 0.2709 - val_loss: 1.8536 - val_accuracy: 0.3358\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9685 - accuracy: 0.2739 - val_loss: 1.8593 - val_accuracy: 0.3384\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9455 - accuracy: 0.2804 - val_loss: 1.8621 - val_accuracy: 0.3351\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9433 - accuracy: 0.2811 - val_loss: 1.8663 - val_accuracy: 0.3361\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.9390 - accuracy: 0.2849 - val_loss: 1.9028 - val_accuracy: 0.3175\n",
      "Epoch 15/100\n",
      "35/40 [=========================>....] - ETA: 0s - loss: 1.9395 - accuracy: 0.2856"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 준비 (예제 데이터 사용)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.reshape(-1, 3072).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 3072).astype('float32') / 255\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential([\n",
    "    Dense(300, activation='relu', input_shape=(3072,)),\n",
    "    Dropout(0.5),  # Dropout 층 추가\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout 층 추가\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train, batch_size=1024, epochs=100, validation_split=0.2)\n",
    "\n",
    "# 검증\n",
    "y_probas = np.stack([model(x_test, training=True)\n",
    "                     for sample in range(50)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test, np.argmax(model.predict(x_test), axis=1)))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "# y_probas = np.stack([mc_model(x_test, training=True)\n",
    "#                      for sample in range(1000)])\n",
    "y_probas = np.stack([mc_model.predict(x_test)\n",
    "                     for sample in range(10)])\n",
    "                     \n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test, np.argmax(mc_model.predict(x_test), axis=1)))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 - 2s - loss: 3.1143 - accuracy: 0.1166 - val_loss: 2.4854 - val_accuracy: 0.1315 - 2s/epoch - 136ms/step\n",
      "Epoch 2/200\n",
      "16/16 - 0s - loss: 2.3339 - accuracy: 0.1628 - val_loss: 2.2541 - val_accuracy: 0.1860 - 279ms/epoch - 17ms/step\n",
      "Epoch 3/200\n",
      "16/16 - 0s - loss: 2.1691 - accuracy: 0.2120 - val_loss: 2.1269 - val_accuracy: 0.2220 - 232ms/epoch - 14ms/step\n",
      "Epoch 4/200\n",
      "16/16 - 0s - loss: 2.0951 - accuracy: 0.2379 - val_loss: 2.0982 - val_accuracy: 0.2280 - 222ms/epoch - 14ms/step\n",
      "Epoch 5/200\n",
      "16/16 - 0s - loss: 2.0404 - accuracy: 0.2526 - val_loss: 2.0090 - val_accuracy: 0.2690 - 219ms/epoch - 14ms/step\n",
      "Epoch 6/200\n",
      "16/16 - 0s - loss: 1.9975 - accuracy: 0.2755 - val_loss: 1.9764 - val_accuracy: 0.2820 - 228ms/epoch - 14ms/step\n",
      "Epoch 7/200\n",
      "16/16 - 0s - loss: 1.9700 - accuracy: 0.2851 - val_loss: 1.9823 - val_accuracy: 0.2830 - 247ms/epoch - 15ms/step\n",
      "Epoch 8/200\n",
      "16/16 - 0s - loss: 1.9397 - accuracy: 0.2980 - val_loss: 1.9876 - val_accuracy: 0.2855 - 236ms/epoch - 15ms/step\n",
      "Epoch 9/200\n",
      "16/16 - 0s - loss: 1.9238 - accuracy: 0.3059 - val_loss: 1.9166 - val_accuracy: 0.3205 - 262ms/epoch - 16ms/step\n",
      "Epoch 10/200\n",
      "16/16 - 0s - loss: 1.8894 - accuracy: 0.3196 - val_loss: 1.8894 - val_accuracy: 0.3250 - 271ms/epoch - 17ms/step\n",
      "Epoch 11/200\n",
      "16/16 - 0s - loss: 1.8696 - accuracy: 0.3285 - val_loss: 1.9628 - val_accuracy: 0.2990 - 252ms/epoch - 16ms/step\n",
      "Epoch 12/200\n",
      "16/16 - 0s - loss: 1.8768 - accuracy: 0.3250 - val_loss: 1.9124 - val_accuracy: 0.3125 - 260ms/epoch - 16ms/step\n",
      "Epoch 13/200\n",
      "16/16 - 0s - loss: 1.8333 - accuracy: 0.3399 - val_loss: 1.8625 - val_accuracy: 0.3205 - 277ms/epoch - 17ms/step\n",
      "Epoch 14/200\n",
      "16/16 - 0s - loss: 1.8215 - accuracy: 0.3431 - val_loss: 1.8648 - val_accuracy: 0.3270 - 290ms/epoch - 18ms/step\n",
      "Epoch 15/200\n",
      "16/16 - 0s - loss: 1.8366 - accuracy: 0.3434 - val_loss: 1.8562 - val_accuracy: 0.3310 - 284ms/epoch - 18ms/step\n",
      "Epoch 16/200\n",
      "16/16 - 0s - loss: 1.7854 - accuracy: 0.3564 - val_loss: 1.8388 - val_accuracy: 0.3355 - 287ms/epoch - 18ms/step\n",
      "Epoch 17/200\n",
      "16/16 - 0s - loss: 1.7783 - accuracy: 0.3564 - val_loss: 1.8768 - val_accuracy: 0.3375 - 328ms/epoch - 21ms/step\n",
      "Epoch 18/200\n",
      "16/16 - 0s - loss: 1.7723 - accuracy: 0.3580 - val_loss: 1.8741 - val_accuracy: 0.3295 - 270ms/epoch - 17ms/step\n",
      "Epoch 19/200\n",
      "16/16 - 0s - loss: 1.7886 - accuracy: 0.3574 - val_loss: 1.8169 - val_accuracy: 0.3495 - 275ms/epoch - 17ms/step\n",
      "Epoch 20/200\n",
      "16/16 - 0s - loss: 1.7394 - accuracy: 0.3765 - val_loss: 1.8905 - val_accuracy: 0.3305 - 273ms/epoch - 17ms/step\n",
      "Epoch 21/200\n",
      "16/16 - 0s - loss: 1.7305 - accuracy: 0.3772 - val_loss: 1.8470 - val_accuracy: 0.3445 - 294ms/epoch - 18ms/step\n",
      "Epoch 22/200\n",
      "16/16 - 0s - loss: 1.7410 - accuracy: 0.3721 - val_loss: 1.8347 - val_accuracy: 0.3575 - 263ms/epoch - 16ms/step\n",
      "Epoch 23/200\n",
      "16/16 - 0s - loss: 1.7054 - accuracy: 0.3901 - val_loss: 1.7954 - val_accuracy: 0.3490 - 277ms/epoch - 17ms/step\n",
      "Epoch 24/200\n",
      "16/16 - 0s - loss: 1.6896 - accuracy: 0.3910 - val_loss: 1.7673 - val_accuracy: 0.3685 - 256ms/epoch - 16ms/step\n",
      "Epoch 25/200\n",
      "16/16 - 0s - loss: 1.6889 - accuracy: 0.3968 - val_loss: 1.8916 - val_accuracy: 0.3425 - 259ms/epoch - 16ms/step\n",
      "Epoch 26/200\n",
      "16/16 - 0s - loss: 1.7215 - accuracy: 0.3816 - val_loss: 1.7838 - val_accuracy: 0.3640 - 273ms/epoch - 17ms/step\n",
      "Epoch 27/200\n",
      "16/16 - 0s - loss: 1.6793 - accuracy: 0.3952 - val_loss: 1.7709 - val_accuracy: 0.3615 - 294ms/epoch - 18ms/step\n",
      "Epoch 28/200\n",
      "16/16 - 0s - loss: 1.6655 - accuracy: 0.4009 - val_loss: 1.7659 - val_accuracy: 0.3625 - 404ms/epoch - 25ms/step\n",
      "Epoch 29/200\n",
      "16/16 - 0s - loss: 1.7010 - accuracy: 0.3909 - val_loss: 1.8009 - val_accuracy: 0.3490 - 264ms/epoch - 17ms/step\n",
      "Epoch 30/200\n",
      "16/16 - 0s - loss: 1.6463 - accuracy: 0.4095 - val_loss: 1.8883 - val_accuracy: 0.3380 - 257ms/epoch - 16ms/step\n",
      "Epoch 31/200\n",
      "16/16 - 0s - loss: 1.6396 - accuracy: 0.4136 - val_loss: 1.7539 - val_accuracy: 0.3575 - 301ms/epoch - 19ms/step\n",
      "Epoch 32/200\n",
      "16/16 - 0s - loss: 1.6370 - accuracy: 0.4137 - val_loss: 1.7450 - val_accuracy: 0.3730 - 289ms/epoch - 18ms/step\n",
      "Epoch 33/200\n",
      "16/16 - 0s - loss: 1.6199 - accuracy: 0.4196 - val_loss: 1.7936 - val_accuracy: 0.3565 - 287ms/epoch - 18ms/step\n",
      "Epoch 34/200\n",
      "16/16 - 0s - loss: 1.6097 - accuracy: 0.4261 - val_loss: 1.7749 - val_accuracy: 0.3535 - 278ms/epoch - 17ms/step\n",
      "Epoch 35/200\n",
      "16/16 - 0s - loss: 1.6113 - accuracy: 0.4257 - val_loss: 1.7532 - val_accuracy: 0.3695 - 265ms/epoch - 17ms/step\n",
      "Epoch 36/200\n",
      "16/16 - 0s - loss: 1.6060 - accuracy: 0.4220 - val_loss: 1.7659 - val_accuracy: 0.3615 - 303ms/epoch - 19ms/step\n",
      "Epoch 37/200\n",
      "16/16 - 0s - loss: 1.5905 - accuracy: 0.4327 - val_loss: 1.8068 - val_accuracy: 0.3520 - 259ms/epoch - 16ms/step\n",
      "Epoch 38/200\n",
      "16/16 - 0s - loss: 1.5900 - accuracy: 0.4255 - val_loss: 1.7391 - val_accuracy: 0.3865 - 335ms/epoch - 21ms/step\n",
      "Epoch 39/200\n",
      "16/16 - 0s - loss: 1.5848 - accuracy: 0.4365 - val_loss: 1.7449 - val_accuracy: 0.3780 - 292ms/epoch - 18ms/step\n",
      "Epoch 40/200\n",
      "16/16 - 0s - loss: 1.5844 - accuracy: 0.4383 - val_loss: 1.7509 - val_accuracy: 0.3775 - 254ms/epoch - 16ms/step\n",
      "Epoch 41/200\n",
      "16/16 - 0s - loss: 1.5725 - accuracy: 0.4350 - val_loss: 1.8089 - val_accuracy: 0.3630 - 279ms/epoch - 17ms/step\n",
      "Epoch 42/200\n",
      "16/16 - 0s - loss: 1.5672 - accuracy: 0.4372 - val_loss: 1.7736 - val_accuracy: 0.3675 - 361ms/epoch - 23ms/step\n",
      "Epoch 43/200\n",
      "16/16 - 0s - loss: 1.5402 - accuracy: 0.4569 - val_loss: 1.7452 - val_accuracy: 0.3770 - 316ms/epoch - 20ms/step\n",
      "Epoch 44/200\n",
      "16/16 - 0s - loss: 1.5410 - accuracy: 0.4502 - val_loss: 1.7692 - val_accuracy: 0.3725 - 264ms/epoch - 16ms/step\n",
      "Epoch 45/200\n",
      "16/16 - 0s - loss: 1.5393 - accuracy: 0.4531 - val_loss: 1.7272 - val_accuracy: 0.3845 - 291ms/epoch - 18ms/step\n",
      "Epoch 46/200\n",
      "16/16 - 0s - loss: 1.5356 - accuracy: 0.4556 - val_loss: 1.8679 - val_accuracy: 0.3430 - 271ms/epoch - 17ms/step\n",
      "Epoch 47/200\n",
      "16/16 - 0s - loss: 1.5337 - accuracy: 0.4545 - val_loss: 1.7285 - val_accuracy: 0.3840 - 417ms/epoch - 26ms/step\n",
      "Epoch 48/200\n",
      "16/16 - 0s - loss: 1.5096 - accuracy: 0.4610 - val_loss: 1.7465 - val_accuracy: 0.3855 - 332ms/epoch - 21ms/step\n",
      "Epoch 49/200\n",
      "16/16 - 0s - loss: 1.4982 - accuracy: 0.4686 - val_loss: 1.7867 - val_accuracy: 0.3685 - 318ms/epoch - 20ms/step\n",
      "Epoch 50/200\n",
      "16/16 - 0s - loss: 1.4971 - accuracy: 0.4716 - val_loss: 1.7542 - val_accuracy: 0.3820 - 299ms/epoch - 19ms/step\n",
      "Epoch 51/200\n",
      "16/16 - 0s - loss: 1.4969 - accuracy: 0.4694 - val_loss: 1.7511 - val_accuracy: 0.3820 - 411ms/epoch - 26ms/step\n",
      "Epoch 52/200\n",
      "16/16 - 0s - loss: 1.4962 - accuracy: 0.4625 - val_loss: 1.7081 - val_accuracy: 0.4005 - 272ms/epoch - 17ms/step\n",
      "Epoch 53/200\n",
      "16/16 - 0s - loss: 1.4859 - accuracy: 0.4767 - val_loss: 1.7409 - val_accuracy: 0.3995 - 425ms/epoch - 27ms/step\n",
      "Epoch 54/200\n",
      "16/16 - 0s - loss: 1.4830 - accuracy: 0.4750 - val_loss: 1.7612 - val_accuracy: 0.3910 - 293ms/epoch - 18ms/step\n",
      "Epoch 55/200\n",
      "16/16 - 0s - loss: 1.4871 - accuracy: 0.4779 - val_loss: 1.7182 - val_accuracy: 0.3965 - 275ms/epoch - 17ms/step\n",
      "Epoch 56/200\n",
      "16/16 - 0s - loss: 1.4472 - accuracy: 0.4871 - val_loss: 1.7796 - val_accuracy: 0.3860 - 261ms/epoch - 16ms/step\n",
      "Epoch 57/200\n",
      "16/16 - 0s - loss: 1.4683 - accuracy: 0.4819 - val_loss: 1.7552 - val_accuracy: 0.3940 - 262ms/epoch - 16ms/step\n",
      "Epoch 58/200\n",
      "16/16 - 0s - loss: 1.4458 - accuracy: 0.4910 - val_loss: 1.7425 - val_accuracy: 0.3965 - 253ms/epoch - 16ms/step\n",
      "Epoch 59/200\n",
      "16/16 - 0s - loss: 1.4463 - accuracy: 0.4854 - val_loss: 1.7395 - val_accuracy: 0.3900 - 256ms/epoch - 16ms/step\n",
      "Epoch 60/200\n",
      "16/16 - 0s - loss: 1.4503 - accuracy: 0.4845 - val_loss: 1.7396 - val_accuracy: 0.3855 - 294ms/epoch - 18ms/step\n",
      "Epoch 61/200\n",
      "16/16 - 0s - loss: 1.4309 - accuracy: 0.4910 - val_loss: 1.8194 - val_accuracy: 0.3865 - 313ms/epoch - 20ms/step\n",
      "Epoch 62/200\n",
      "16/16 - 0s - loss: 1.4401 - accuracy: 0.4899 - val_loss: 1.7350 - val_accuracy: 0.3900 - 277ms/epoch - 17ms/step\n",
      "Epoch 63/200\n",
      "16/16 - 0s - loss: 1.4223 - accuracy: 0.5008 - val_loss: 1.7502 - val_accuracy: 0.3920 - 279ms/epoch - 17ms/step\n",
      "Epoch 64/200\n",
      "16/16 - 0s - loss: 1.4147 - accuracy: 0.4954 - val_loss: 1.7404 - val_accuracy: 0.3975 - 260ms/epoch - 16ms/step\n",
      "Epoch 65/200\n",
      "16/16 - 0s - loss: 1.4129 - accuracy: 0.4970 - val_loss: 1.7448 - val_accuracy: 0.3930 - 330ms/epoch - 21ms/step\n",
      "Epoch 66/200\n",
      "16/16 - 0s - loss: 1.4074 - accuracy: 0.5024 - val_loss: 1.7839 - val_accuracy: 0.3815 - 266ms/epoch - 17ms/step\n",
      "Epoch 67/200\n",
      "16/16 - 0s - loss: 1.4056 - accuracy: 0.5073 - val_loss: 1.7585 - val_accuracy: 0.3935 - 309ms/epoch - 19ms/step\n",
      "Epoch 68/200\n",
      "16/16 - 0s - loss: 1.3849 - accuracy: 0.5120 - val_loss: 1.7549 - val_accuracy: 0.3955 - 260ms/epoch - 16ms/step\n",
      "Epoch 69/200\n",
      "16/16 - 0s - loss: 1.3777 - accuracy: 0.5159 - val_loss: 1.7521 - val_accuracy: 0.3910 - 286ms/epoch - 18ms/step\n",
      "Epoch 70/200\n",
      "16/16 - 0s - loss: 1.3775 - accuracy: 0.5106 - val_loss: 1.7786 - val_accuracy: 0.3970 - 301ms/epoch - 19ms/step\n",
      "Epoch 71/200\n",
      "16/16 - 0s - loss: 1.3828 - accuracy: 0.5110 - val_loss: 1.7428 - val_accuracy: 0.4060 - 316ms/epoch - 20ms/step\n",
      "Epoch 72/200\n",
      "16/16 - 0s - loss: 1.3465 - accuracy: 0.5257 - val_loss: 1.7408 - val_accuracy: 0.3950 - 263ms/epoch - 16ms/step\n",
      "Epoch 73/200\n",
      "16/16 - 0s - loss: 1.3434 - accuracy: 0.5269 - val_loss: 1.7325 - val_accuracy: 0.3985 - 249ms/epoch - 16ms/step\n",
      "Epoch 74/200\n",
      "16/16 - 0s - loss: 1.3487 - accuracy: 0.5211 - val_loss: 1.7491 - val_accuracy: 0.4025 - 307ms/epoch - 19ms/step\n",
      "Epoch 75/200\n",
      "16/16 - 0s - loss: 1.3373 - accuracy: 0.5260 - val_loss: 1.7567 - val_accuracy: 0.4005 - 267ms/epoch - 17ms/step\n",
      "Epoch 76/200\n",
      "16/16 - 0s - loss: 1.3790 - accuracy: 0.5114 - val_loss: 1.7926 - val_accuracy: 0.3840 - 277ms/epoch - 17ms/step\n",
      "Epoch 77/200\n",
      "16/16 - 0s - loss: 1.3515 - accuracy: 0.5213 - val_loss: 1.7419 - val_accuracy: 0.4060 - 274ms/epoch - 17ms/step\n",
      "Epoch 78/200\n",
      "16/16 - 0s - loss: 1.3208 - accuracy: 0.5319 - val_loss: 1.7579 - val_accuracy: 0.4000 - 426ms/epoch - 27ms/step\n",
      "Epoch 79/200\n",
      "16/16 - 0s - loss: 1.3145 - accuracy: 0.5328 - val_loss: 1.8099 - val_accuracy: 0.3980 - 327ms/epoch - 20ms/step\n",
      "Epoch 80/200\n",
      "16/16 - 0s - loss: 1.2988 - accuracy: 0.5416 - val_loss: 1.7435 - val_accuracy: 0.4085 - 338ms/epoch - 21ms/step\n",
      "Epoch 81/200\n",
      "16/16 - 0s - loss: 1.2991 - accuracy: 0.5389 - val_loss: 1.7725 - val_accuracy: 0.4115 - 312ms/epoch - 19ms/step\n",
      "Epoch 82/200\n",
      "16/16 - 0s - loss: 1.3370 - accuracy: 0.5275 - val_loss: 1.8334 - val_accuracy: 0.3775 - 295ms/epoch - 18ms/step\n",
      "Epoch 83/200\n",
      "16/16 - 0s - loss: 1.3142 - accuracy: 0.5383 - val_loss: 1.7775 - val_accuracy: 0.3860 - 314ms/epoch - 20ms/step\n",
      "Epoch 84/200\n",
      "16/16 - 0s - loss: 1.3257 - accuracy: 0.5346 - val_loss: 1.8031 - val_accuracy: 0.3880 - 351ms/epoch - 22ms/step\n",
      "Epoch 85/200\n",
      "16/16 - 0s - loss: 1.2973 - accuracy: 0.5416 - val_loss: 1.7366 - val_accuracy: 0.4040 - 274ms/epoch - 17ms/step\n",
      "Epoch 86/200\n",
      "16/16 - 0s - loss: 1.3089 - accuracy: 0.5357 - val_loss: 1.7724 - val_accuracy: 0.4055 - 276ms/epoch - 17ms/step\n",
      "Epoch 87/200\n",
      "16/16 - 0s - loss: 1.2910 - accuracy: 0.5399 - val_loss: 1.7326 - val_accuracy: 0.4025 - 286ms/epoch - 18ms/step\n",
      "Epoch 88/200\n",
      "16/16 - 0s - loss: 1.2654 - accuracy: 0.5543 - val_loss: 1.7594 - val_accuracy: 0.4040 - 275ms/epoch - 17ms/step\n",
      "Epoch 89/200\n",
      "16/16 - 0s - loss: 1.2587 - accuracy: 0.5537 - val_loss: 1.7839 - val_accuracy: 0.3950 - 390ms/epoch - 24ms/step\n",
      "Epoch 90/200\n",
      "16/16 - 0s - loss: 1.2832 - accuracy: 0.5461 - val_loss: 1.7565 - val_accuracy: 0.4055 - 420ms/epoch - 26ms/step\n",
      "Epoch 91/200\n",
      "16/16 - 0s - loss: 1.2527 - accuracy: 0.5579 - val_loss: 1.7545 - val_accuracy: 0.4055 - 293ms/epoch - 18ms/step\n",
      "Epoch 92/200\n",
      "16/16 - 0s - loss: 1.2499 - accuracy: 0.5543 - val_loss: 1.8301 - val_accuracy: 0.3910 - 286ms/epoch - 18ms/step\n",
      "Epoch 93/200\n",
      "16/16 - 0s - loss: 1.2473 - accuracy: 0.5608 - val_loss: 1.8663 - val_accuracy: 0.3940 - 307ms/epoch - 19ms/step\n",
      "Epoch 94/200\n",
      "16/16 - 0s - loss: 1.2497 - accuracy: 0.5602 - val_loss: 1.8178 - val_accuracy: 0.3965 - 368ms/epoch - 23ms/step\n",
      "Epoch 95/200\n",
      "16/16 - 0s - loss: 1.2319 - accuracy: 0.5702 - val_loss: 1.8064 - val_accuracy: 0.3860 - 381ms/epoch - 24ms/step\n",
      "Epoch 96/200\n",
      "16/16 - 0s - loss: 1.2204 - accuracy: 0.5754 - val_loss: 1.8428 - val_accuracy: 0.3955 - 438ms/epoch - 27ms/step\n",
      "Epoch 97/200\n",
      "16/16 - 0s - loss: 1.2225 - accuracy: 0.5649 - val_loss: 1.8076 - val_accuracy: 0.4075 - 313ms/epoch - 20ms/step\n",
      "Epoch 98/200\n",
      "16/16 - 0s - loss: 1.2127 - accuracy: 0.5757 - val_loss: 1.7496 - val_accuracy: 0.4155 - 319ms/epoch - 20ms/step\n",
      "Epoch 99/200\n",
      "16/16 - 0s - loss: 1.2478 - accuracy: 0.5606 - val_loss: 1.7978 - val_accuracy: 0.4005 - 352ms/epoch - 22ms/step\n",
      "Epoch 100/200\n",
      "16/16 - 0s - loss: 1.1906 - accuracy: 0.5815 - val_loss: 1.7725 - val_accuracy: 0.4085 - 287ms/epoch - 18ms/step\n",
      "Epoch 101/200\n",
      "16/16 - 0s - loss: 1.2144 - accuracy: 0.5715 - val_loss: 1.8040 - val_accuracy: 0.4030 - 292ms/epoch - 18ms/step\n",
      "Epoch 102/200\n",
      "16/16 - 0s - loss: 1.2074 - accuracy: 0.5754 - val_loss: 1.7862 - val_accuracy: 0.4135 - 291ms/epoch - 18ms/step\n",
      "Epoch 103/200\n",
      "16/16 - 0s - loss: 1.2090 - accuracy: 0.5700 - val_loss: 1.8073 - val_accuracy: 0.4065 - 388ms/epoch - 24ms/step\n",
      "Epoch 104/200\n",
      "16/16 - 0s - loss: 1.1915 - accuracy: 0.5846 - val_loss: 1.8539 - val_accuracy: 0.3885 - 308ms/epoch - 19ms/step\n",
      "Epoch 105/200\n",
      "16/16 - 0s - loss: 1.1705 - accuracy: 0.5879 - val_loss: 1.8020 - val_accuracy: 0.4060 - 320ms/epoch - 20ms/step\n",
      "Epoch 106/200\n",
      "16/16 - 0s - loss: 1.1695 - accuracy: 0.5920 - val_loss: 1.8184 - val_accuracy: 0.3950 - 328ms/epoch - 21ms/step\n",
      "Epoch 107/200\n",
      "16/16 - 0s - loss: 1.1894 - accuracy: 0.5894 - val_loss: 1.8303 - val_accuracy: 0.4070 - 312ms/epoch - 20ms/step\n",
      "Epoch 108/200\n",
      "16/16 - 0s - loss: 1.1636 - accuracy: 0.5894 - val_loss: 1.8942 - val_accuracy: 0.3820 - 319ms/epoch - 20ms/step\n",
      "Epoch 109/200\n",
      "16/16 - 0s - loss: 1.1594 - accuracy: 0.5934 - val_loss: 1.8212 - val_accuracy: 0.4030 - 302ms/epoch - 19ms/step\n",
      "Epoch 110/200\n",
      "16/16 - 0s - loss: 1.1565 - accuracy: 0.5943 - val_loss: 1.8175 - val_accuracy: 0.4075 - 296ms/epoch - 18ms/step\n",
      "Epoch 111/200\n",
      "16/16 - 0s - loss: 1.1650 - accuracy: 0.5922 - val_loss: 1.8977 - val_accuracy: 0.3855 - 286ms/epoch - 18ms/step\n",
      "Epoch 112/200\n",
      "16/16 - 0s - loss: 1.1463 - accuracy: 0.5919 - val_loss: 1.8304 - val_accuracy: 0.4125 - 280ms/epoch - 17ms/step\n",
      "Epoch 113/200\n",
      "16/16 - 0s - loss: 1.1537 - accuracy: 0.5971 - val_loss: 1.8024 - val_accuracy: 0.4055 - 278ms/epoch - 17ms/step\n",
      "Epoch 114/200\n",
      "16/16 - 0s - loss: 1.1260 - accuracy: 0.6081 - val_loss: 1.8584 - val_accuracy: 0.4060 - 277ms/epoch - 17ms/step\n",
      "Epoch 115/200\n",
      "16/16 - 0s - loss: 1.1163 - accuracy: 0.6124 - val_loss: 1.8626 - val_accuracy: 0.3990 - 330ms/epoch - 21ms/step\n",
      "Epoch 116/200\n",
      "16/16 - 0s - loss: 1.1080 - accuracy: 0.6175 - val_loss: 1.9209 - val_accuracy: 0.3890 - 333ms/epoch - 21ms/step\n",
      "Epoch 117/200\n",
      "16/16 - 0s - loss: 1.1449 - accuracy: 0.5960 - val_loss: 1.8651 - val_accuracy: 0.3935 - 318ms/epoch - 20ms/step\n",
      "Epoch 118/200\n",
      "16/16 - 0s - loss: 1.1003 - accuracy: 0.6149 - val_loss: 1.8947 - val_accuracy: 0.3910 - 318ms/epoch - 20ms/step\n",
      "Epoch 119/200\n",
      "16/16 - 0s - loss: 1.1285 - accuracy: 0.6074 - val_loss: 1.8818 - val_accuracy: 0.3915 - 296ms/epoch - 19ms/step\n",
      "Epoch 120/200\n",
      "16/16 - 0s - loss: 1.1289 - accuracy: 0.6018 - val_loss: 1.8517 - val_accuracy: 0.3995 - 300ms/epoch - 19ms/step\n",
      "Epoch 121/200\n",
      "16/16 - 0s - loss: 1.0829 - accuracy: 0.6194 - val_loss: 1.8570 - val_accuracy: 0.4050 - 309ms/epoch - 19ms/step\n",
      "Epoch 122/200\n",
      "16/16 - 0s - loss: 1.1088 - accuracy: 0.6124 - val_loss: 1.8354 - val_accuracy: 0.4120 - 322ms/epoch - 20ms/step\n",
      "Epoch 123/200\n",
      "16/16 - 0s - loss: 1.0686 - accuracy: 0.6340 - val_loss: 1.9022 - val_accuracy: 0.4010 - 302ms/epoch - 19ms/step\n",
      "Epoch 124/200\n",
      "16/16 - 0s - loss: 1.1125 - accuracy: 0.6045 - val_loss: 1.8635 - val_accuracy: 0.4000 - 337ms/epoch - 21ms/step\n",
      "Epoch 125/200\n",
      "16/16 - 0s - loss: 1.0581 - accuracy: 0.6329 - val_loss: 1.8688 - val_accuracy: 0.4080 - 312ms/epoch - 20ms/step\n",
      "Epoch 126/200\n",
      "16/16 - 0s - loss: 1.0691 - accuracy: 0.6315 - val_loss: 1.9005 - val_accuracy: 0.4060 - 310ms/epoch - 19ms/step\n",
      "Epoch 127/200\n",
      "16/16 - 0s - loss: 1.0562 - accuracy: 0.6309 - val_loss: 1.8828 - val_accuracy: 0.4080 - 334ms/epoch - 21ms/step\n",
      "Epoch 128/200\n",
      "16/16 - 0s - loss: 1.0449 - accuracy: 0.6364 - val_loss: 1.8846 - val_accuracy: 0.4090 - 456ms/epoch - 28ms/step\n",
      "Epoch 129/200\n",
      "16/16 - 0s - loss: 1.0681 - accuracy: 0.6225 - val_loss: 1.8897 - val_accuracy: 0.4055 - 306ms/epoch - 19ms/step\n",
      "Epoch 130/200\n",
      "16/16 - 0s - loss: 1.0298 - accuracy: 0.6451 - val_loss: 1.9288 - val_accuracy: 0.3925 - 312ms/epoch - 20ms/step\n",
      "Epoch 131/200\n",
      "16/16 - 0s - loss: 1.0346 - accuracy: 0.6388 - val_loss: 1.8936 - val_accuracy: 0.4090 - 321ms/epoch - 20ms/step\n",
      "Epoch 132/200\n",
      "16/16 - 0s - loss: 1.0370 - accuracy: 0.6391 - val_loss: 1.9438 - val_accuracy: 0.3935 - 297ms/epoch - 19ms/step\n",
      "Epoch 133/200\n",
      "16/16 - 0s - loss: 1.0310 - accuracy: 0.6435 - val_loss: 1.9796 - val_accuracy: 0.3900 - 390ms/epoch - 24ms/step\n",
      "Epoch 134/200\n",
      "16/16 - 0s - loss: 1.0309 - accuracy: 0.6392 - val_loss: 1.9552 - val_accuracy: 0.3855 - 324ms/epoch - 20ms/step\n",
      "Epoch 135/200\n",
      "16/16 - 0s - loss: 1.0684 - accuracy: 0.6252 - val_loss: 1.8961 - val_accuracy: 0.3925 - 328ms/epoch - 21ms/step\n",
      "Epoch 136/200\n",
      "16/16 - 0s - loss: 1.0110 - accuracy: 0.6451 - val_loss: 1.9451 - val_accuracy: 0.4055 - 326ms/epoch - 20ms/step\n",
      "Epoch 137/200\n",
      "16/16 - 0s - loss: 1.0047 - accuracy: 0.6484 - val_loss: 1.9667 - val_accuracy: 0.3935 - 407ms/epoch - 25ms/step\n",
      "Epoch 138/200\n",
      "16/16 - 0s - loss: 1.0130 - accuracy: 0.6506 - val_loss: 1.9880 - val_accuracy: 0.3945 - 336ms/epoch - 21ms/step\n",
      "Epoch 139/200\n",
      "16/16 - 0s - loss: 0.9805 - accuracy: 0.6672 - val_loss: 2.0421 - val_accuracy: 0.3930 - 414ms/epoch - 26ms/step\n",
      "Epoch 140/200\n",
      "16/16 - 0s - loss: 1.0327 - accuracy: 0.6396 - val_loss: 2.0218 - val_accuracy: 0.3870 - 360ms/epoch - 23ms/step\n",
      "Epoch 141/200\n",
      "16/16 - 0s - loss: 0.9924 - accuracy: 0.6589 - val_loss: 2.0010 - val_accuracy: 0.3885 - 341ms/epoch - 21ms/step\n",
      "Epoch 142/200\n",
      "16/16 - 0s - loss: 0.9879 - accuracy: 0.6597 - val_loss: 1.9487 - val_accuracy: 0.4050 - 352ms/epoch - 22ms/step\n",
      "Epoch 143/200\n",
      "16/16 - 0s - loss: 0.9641 - accuracy: 0.6671 - val_loss: 1.9432 - val_accuracy: 0.4060 - 338ms/epoch - 21ms/step\n",
      "Epoch 144/200\n",
      "16/16 - 0s - loss: 0.9830 - accuracy: 0.6586 - val_loss: 1.9926 - val_accuracy: 0.4105 - 305ms/epoch - 19ms/step\n",
      "Epoch 145/200\n",
      "16/16 - 0s - loss: 0.9742 - accuracy: 0.6654 - val_loss: 2.0125 - val_accuracy: 0.4045 - 312ms/epoch - 20ms/step\n",
      "Epoch 146/200\n",
      "16/16 - 0s - loss: 0.9743 - accuracy: 0.6662 - val_loss: 2.0289 - val_accuracy: 0.3885 - 326ms/epoch - 20ms/step\n",
      "Epoch 147/200\n",
      "16/16 - 0s - loss: 0.9775 - accuracy: 0.6641 - val_loss: 2.0124 - val_accuracy: 0.3995 - 272ms/epoch - 17ms/step\n",
      "Epoch 148/200\n",
      "16/16 - 0s - loss: 0.9570 - accuracy: 0.6722 - val_loss: 2.0470 - val_accuracy: 0.3895 - 301ms/epoch - 19ms/step\n",
      "Epoch 149/200\n",
      "16/16 - 0s - loss: 0.9804 - accuracy: 0.6565 - val_loss: 2.0252 - val_accuracy: 0.3760 - 302ms/epoch - 19ms/step\n",
      "Epoch 150/200\n",
      "16/16 - 0s - loss: 0.9470 - accuracy: 0.6727 - val_loss: 2.0405 - val_accuracy: 0.3885 - 341ms/epoch - 21ms/step\n",
      "Epoch 151/200\n",
      "16/16 - 0s - loss: 0.9530 - accuracy: 0.6699 - val_loss: 2.0045 - val_accuracy: 0.3985 - 279ms/epoch - 17ms/step\n",
      "Epoch 152/200\n",
      "16/16 - 0s - loss: 0.9440 - accuracy: 0.6695 - val_loss: 1.9804 - val_accuracy: 0.4035 - 325ms/epoch - 20ms/step\n",
      "Epoch 153/200\n",
      "16/16 - 0s - loss: 0.8999 - accuracy: 0.6896 - val_loss: 1.9888 - val_accuracy: 0.4040 - 371ms/epoch - 23ms/step\n",
      "Epoch 154/200\n",
      "16/16 - 0s - loss: 0.9060 - accuracy: 0.6936 - val_loss: 2.0397 - val_accuracy: 0.3930 - 319ms/epoch - 20ms/step\n",
      "Epoch 155/200\n",
      "16/16 - 0s - loss: 0.9731 - accuracy: 0.6626 - val_loss: 2.0291 - val_accuracy: 0.3935 - 295ms/epoch - 18ms/step\n",
      "Epoch 156/200\n",
      "16/16 - 0s - loss: 0.9369 - accuracy: 0.6812 - val_loss: 2.0434 - val_accuracy: 0.4100 - 313ms/epoch - 20ms/step\n",
      "Epoch 157/200\n",
      "16/16 - 0s - loss: 0.9435 - accuracy: 0.6730 - val_loss: 2.0536 - val_accuracy: 0.3985 - 320ms/epoch - 20ms/step\n",
      "Epoch 158/200\n",
      "16/16 - 0s - loss: 0.9428 - accuracy: 0.6670 - val_loss: 2.0387 - val_accuracy: 0.3995 - 293ms/epoch - 18ms/step\n",
      "Epoch 159/200\n",
      "16/16 - 0s - loss: 0.8781 - accuracy: 0.7040 - val_loss: 2.0839 - val_accuracy: 0.3860 - 289ms/epoch - 18ms/step\n",
      "Epoch 160/200\n",
      "16/16 - 0s - loss: 0.8697 - accuracy: 0.7056 - val_loss: 2.0561 - val_accuracy: 0.3965 - 294ms/epoch - 18ms/step\n",
      "Epoch 161/200\n",
      "16/16 - 0s - loss: 0.8931 - accuracy: 0.6877 - val_loss: 2.0409 - val_accuracy: 0.3880 - 307ms/epoch - 19ms/step\n",
      "Epoch 162/200\n",
      "16/16 - 0s - loss: 0.8607 - accuracy: 0.7032 - val_loss: 2.0641 - val_accuracy: 0.3910 - 330ms/epoch - 21ms/step\n",
      "Epoch 163/200\n",
      "16/16 - 0s - loss: 0.8902 - accuracy: 0.6946 - val_loss: 2.0563 - val_accuracy: 0.4055 - 321ms/epoch - 20ms/step\n",
      "Epoch 164/200\n",
      "16/16 - 0s - loss: 0.8599 - accuracy: 0.7092 - val_loss: 2.0471 - val_accuracy: 0.3940 - 295ms/epoch - 18ms/step\n",
      "Epoch 165/200\n",
      "16/16 - 0s - loss: 0.8807 - accuracy: 0.6946 - val_loss: 2.1551 - val_accuracy: 0.3855 - 304ms/epoch - 19ms/step\n",
      "Epoch 166/200\n",
      "16/16 - 0s - loss: 0.8611 - accuracy: 0.7028 - val_loss: 2.0193 - val_accuracy: 0.3960 - 341ms/epoch - 21ms/step\n",
      "Epoch 167/200\n",
      "16/16 - 0s - loss: 0.8457 - accuracy: 0.7089 - val_loss: 2.1107 - val_accuracy: 0.4085 - 346ms/epoch - 22ms/step\n",
      "Epoch 168/200\n",
      "16/16 - 0s - loss: 0.8691 - accuracy: 0.7015 - val_loss: 2.0597 - val_accuracy: 0.4075 - 328ms/epoch - 21ms/step\n",
      "Epoch 169/200\n",
      "16/16 - 0s - loss: 0.8418 - accuracy: 0.7156 - val_loss: 2.0942 - val_accuracy: 0.4020 - 318ms/epoch - 20ms/step\n",
      "Epoch 170/200\n",
      "16/16 - 0s - loss: 0.8399 - accuracy: 0.7129 - val_loss: 2.1031 - val_accuracy: 0.3925 - 386ms/epoch - 24ms/step\n",
      "Epoch 171/200\n",
      "16/16 - 0s - loss: 0.8537 - accuracy: 0.7070 - val_loss: 2.2105 - val_accuracy: 0.3715 - 315ms/epoch - 20ms/step\n",
      "Epoch 172/200\n",
      "16/16 - 0s - loss: 0.8555 - accuracy: 0.7064 - val_loss: 2.1194 - val_accuracy: 0.3900 - 331ms/epoch - 21ms/step\n",
      "Epoch 173/200\n",
      "16/16 - 0s - loss: 0.8592 - accuracy: 0.7078 - val_loss: 2.1493 - val_accuracy: 0.3950 - 393ms/epoch - 25ms/step\n",
      "Epoch 174/200\n",
      "16/16 - 0s - loss: 0.8818 - accuracy: 0.6936 - val_loss: 2.1165 - val_accuracy: 0.3985 - 330ms/epoch - 21ms/step\n",
      "Epoch 175/200\n",
      "16/16 - 0s - loss: 0.8431 - accuracy: 0.7144 - val_loss: 2.0910 - val_accuracy: 0.4030 - 343ms/epoch - 21ms/step\n",
      "Epoch 176/200\n",
      "16/16 - 0s - loss: 0.7800 - accuracy: 0.7416 - val_loss: 2.1480 - val_accuracy: 0.3930 - 376ms/epoch - 23ms/step\n",
      "Epoch 177/200\n",
      "16/16 - 0s - loss: 0.8052 - accuracy: 0.7276 - val_loss: 2.1645 - val_accuracy: 0.3955 - 354ms/epoch - 22ms/step\n",
      "Epoch 178/200\n",
      "16/16 - 1s - loss: 0.8523 - accuracy: 0.7049 - val_loss: 2.1282 - val_accuracy: 0.3950 - 567ms/epoch - 35ms/step\n",
      "Epoch 179/200\n",
      "16/16 - 0s - loss: 0.7916 - accuracy: 0.7349 - val_loss: 2.2347 - val_accuracy: 0.4050 - 341ms/epoch - 21ms/step\n",
      "Epoch 180/200\n",
      "16/16 - 0s - loss: 0.8406 - accuracy: 0.7107 - val_loss: 2.1327 - val_accuracy: 0.3975 - 334ms/epoch - 21ms/step\n",
      "Epoch 181/200\n",
      "16/16 - 0s - loss: 0.7789 - accuracy: 0.7351 - val_loss: 2.1919 - val_accuracy: 0.4045 - 371ms/epoch - 23ms/step\n",
      "Epoch 182/200\n",
      "16/16 - 0s - loss: 0.7862 - accuracy: 0.7331 - val_loss: 2.1858 - val_accuracy: 0.4105 - 321ms/epoch - 20ms/step\n",
      "Epoch 183/200\n",
      "16/16 - 0s - loss: 0.7998 - accuracy: 0.7272 - val_loss: 2.1983 - val_accuracy: 0.4000 - 336ms/epoch - 21ms/step\n",
      "Epoch 184/200\n",
      "16/16 - 0s - loss: 0.7715 - accuracy: 0.7435 - val_loss: 2.1707 - val_accuracy: 0.3820 - 312ms/epoch - 19ms/step\n",
      "Epoch 185/200\n",
      "16/16 - 0s - loss: 0.7603 - accuracy: 0.7387 - val_loss: 2.1589 - val_accuracy: 0.3985 - 320ms/epoch - 20ms/step\n",
      "Epoch 186/200\n",
      "16/16 - 0s - loss: 0.7559 - accuracy: 0.7424 - val_loss: 2.2226 - val_accuracy: 0.4010 - 337ms/epoch - 21ms/step\n",
      "Epoch 187/200\n",
      "16/16 - 0s - loss: 0.7701 - accuracy: 0.7410 - val_loss: 2.2002 - val_accuracy: 0.4065 - 339ms/epoch - 21ms/step\n",
      "Epoch 188/200\n",
      "16/16 - 0s - loss: 0.7542 - accuracy: 0.7434 - val_loss: 2.3020 - val_accuracy: 0.3865 - 362ms/epoch - 23ms/step\n",
      "Epoch 189/200\n",
      "16/16 - 0s - loss: 0.7370 - accuracy: 0.7545 - val_loss: 2.2637 - val_accuracy: 0.3960 - 328ms/epoch - 21ms/step\n",
      "Epoch 190/200\n",
      "16/16 - 0s - loss: 0.7640 - accuracy: 0.7385 - val_loss: 2.2666 - val_accuracy: 0.3955 - 348ms/epoch - 22ms/step\n",
      "Epoch 191/200\n",
      "16/16 - 0s - loss: 0.7312 - accuracy: 0.7536 - val_loss: 2.2702 - val_accuracy: 0.3925 - 311ms/epoch - 19ms/step\n",
      "Epoch 192/200\n",
      "16/16 - 0s - loss: 0.7832 - accuracy: 0.7305 - val_loss: 2.2494 - val_accuracy: 0.4025 - 389ms/epoch - 24ms/step\n",
      "Epoch 193/200\n",
      "16/16 - 0s - loss: 0.7373 - accuracy: 0.7541 - val_loss: 2.2453 - val_accuracy: 0.4050 - 304ms/epoch - 19ms/step\n",
      "Epoch 194/200\n",
      "16/16 - 0s - loss: 0.7338 - accuracy: 0.7544 - val_loss: 2.3344 - val_accuracy: 0.3870 - 335ms/epoch - 21ms/step\n",
      "Epoch 195/200\n",
      "16/16 - 0s - loss: 0.7099 - accuracy: 0.7604 - val_loss: 2.2373 - val_accuracy: 0.3980 - 373ms/epoch - 23ms/step\n",
      "Epoch 196/200\n",
      "16/16 - 0s - loss: 0.7611 - accuracy: 0.7408 - val_loss: 2.3198 - val_accuracy: 0.3970 - 311ms/epoch - 19ms/step\n",
      "Epoch 197/200\n",
      "16/16 - 0s - loss: 0.7056 - accuracy: 0.7639 - val_loss: 2.2494 - val_accuracy: 0.4080 - 315ms/epoch - 20ms/step\n",
      "Epoch 198/200\n",
      "16/16 - 0s - loss: 0.6958 - accuracy: 0.7717 - val_loss: 2.3237 - val_accuracy: 0.3860 - 359ms/epoch - 22ms/step\n",
      "Epoch 199/200\n",
      "16/16 - 0s - loss: 0.7222 - accuracy: 0.7549 - val_loss: 2.3288 - val_accuracy: 0.4100 - 341ms/epoch - 21ms/step\n",
      "Epoch 200/200\n",
      "16/16 - 0s - loss: 0.7240 - accuracy: 0.7525 - val_loss: 2.3105 - val_accuracy: 0.3915 - 354ms/epoch - 22ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[2000:10000]/255\n",
    "y_train = y_train_full[2000:10000].flatten()\n",
    "X_valid = X_train_full[:2000]/255\n",
    "y_valid = y_train_full[:2000].flatten()\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "callbacks = [early_stopping_cb]\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=512, validation_data=(X_valid, y_valid), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, math\n",
    "from tensorflow import keras\n",
    "\n",
    "def get_run_logdir(idx):\n",
    "    return os.path.join(os.getcwd(), 'my_board', f'run_{idx:02}')\n",
    "\n",
    "\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[2000:10000]/255\n",
    "y_train = y_train_full[2000:10000].flatten()\n",
    "X_valid = X_train_full[:2000]/255\n",
    "y_valid = y_train_full[:2000].flatten()\n",
    "X_test = X_test_full/255\n",
    "y_test = y_test_full.flatten()\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 4s - loss: 14.4061 - accuracy: 0.0981 - val_loss: 3.9535 - val_accuracy: 0.1040 - 4s/epoch - 230ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 7.3103 - accuracy: 0.1007 - val_loss: 5.4795 - val_accuracy: 0.1015 - 906ms/epoch - 57ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 5.1126 - accuracy: 0.1016 - val_loss: 5.0060 - val_accuracy: 0.0995 - 910ms/epoch - 57ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 4.4764 - accuracy: 0.1006 - val_loss: 5.2232 - val_accuracy: 0.1015 - 925ms/epoch - 58ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 3.4890 - accuracy: 0.0949 - val_loss: 2.5262 - val_accuracy: 0.1015 - 917ms/epoch - 57ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 2.4408 - accuracy: 0.1070 - val_loss: 2.2713 - val_accuracy: 0.1730 - 921ms/epoch - 58ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 2.3341 - accuracy: 0.1175 - val_loss: 2.2820 - val_accuracy: 0.1605 - 917ms/epoch - 57ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 2.2883 - accuracy: 0.1396 - val_loss: 2.3412 - val_accuracy: 0.1060 - 927ms/epoch - 58ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 2.2913 - accuracy: 0.1312 - val_loss: 2.2188 - val_accuracy: 0.1650 - 942ms/epoch - 59ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 2.2621 - accuracy: 0.1514 - val_loss: 2.1787 - val_accuracy: 0.1510 - 934ms/epoch - 58ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 2.2033 - accuracy: 0.1667 - val_loss: 2.1864 - val_accuracy: 0.1835 - 945ms/epoch - 59ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 2.1616 - accuracy: 0.1844 - val_loss: 2.0850 - val_accuracy: 0.2200 - 934ms/epoch - 58ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 2.1927 - accuracy: 0.1689 - val_loss: 2.1395 - val_accuracy: 0.1840 - 939ms/epoch - 59ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 2.1308 - accuracy: 0.1924 - val_loss: 2.0935 - val_accuracy: 0.2340 - 913ms/epoch - 57ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 2.1452 - accuracy: 0.1883 - val_loss: 2.1522 - val_accuracy: 0.1800 - 943ms/epoch - 59ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 2.0695 - accuracy: 0.2205 - val_loss: 2.2019 - val_accuracy: 0.1700 - 925ms/epoch - 58ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 2.1231 - accuracy: 0.2074 - val_loss: 2.0309 - val_accuracy: 0.2275 - 929ms/epoch - 58ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 2.0410 - accuracy: 0.2298 - val_loss: 2.0020 - val_accuracy: 0.2490 - 925ms/epoch - 58ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 2.0277 - accuracy: 0.2479 - val_loss: 2.0420 - val_accuracy: 0.2515 - 926ms/epoch - 58ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 2.0111 - accuracy: 0.2510 - val_loss: 2.0906 - val_accuracy: 0.2215 - 929ms/epoch - 58ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 2.0212 - accuracy: 0.2516 - val_loss: 1.9385 - val_accuracy: 0.2795 - 915ms/epoch - 57ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 1.9993 - accuracy: 0.2596 - val_loss: 1.9449 - val_accuracy: 0.2780 - 921ms/epoch - 58ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 1.9275 - accuracy: 0.2860 - val_loss: 1.9032 - val_accuracy: 0.3020 - 921ms/epoch - 58ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 1.9482 - accuracy: 0.2835 - val_loss: 2.0137 - val_accuracy: 0.2720 - 929ms/epoch - 58ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 1.9567 - accuracy: 0.2792 - val_loss: 1.9897 - val_accuracy: 0.2645 - 936ms/epoch - 59ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 1.9237 - accuracy: 0.2977 - val_loss: 1.9557 - val_accuracy: 0.2900 - 949ms/epoch - 59ms/step\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 1.9613 - accuracy: 0.2828 - val_loss: 1.9582 - val_accuracy: 0.2715 - 930ms/epoch - 58ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 1.8883 - accuracy: 0.3084 - val_loss: 1.9980 - val_accuracy: 0.2550 - 916ms/epoch - 57ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 1.9387 - accuracy: 0.2982 - val_loss: 1.8851 - val_accuracy: 0.3280 - 918ms/epoch - 57ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 1s - loss: 1.8279 - accuracy: 0.3374 - val_loss: 1.8741 - val_accuracy: 0.3095 - 918ms/epoch - 57ms/step\n",
      "Epoch 31/100\n",
      "16/16 - 1s - loss: 1.9066 - accuracy: 0.3040 - val_loss: 1.9500 - val_accuracy: 0.3070 - 936ms/epoch - 59ms/step\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 1.8864 - accuracy: 0.3101 - val_loss: 1.8691 - val_accuracy: 0.3000 - 931ms/epoch - 58ms/step\n",
      "Epoch 33/100\n",
      "16/16 - 1s - loss: 1.8355 - accuracy: 0.3269 - val_loss: 1.8450 - val_accuracy: 0.3250 - 922ms/epoch - 58ms/step\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 1.8211 - accuracy: 0.3395 - val_loss: 1.9961 - val_accuracy: 0.2660 - 937ms/epoch - 59ms/step\n",
      "Epoch 35/100\n",
      "16/16 - 1s - loss: 1.8432 - accuracy: 0.3325 - val_loss: 1.8417 - val_accuracy: 0.3390 - 933ms/epoch - 58ms/step\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 1.8454 - accuracy: 0.3240 - val_loss: 1.8117 - val_accuracy: 0.3430 - 937ms/epoch - 59ms/step\n",
      "Epoch 37/100\n",
      "16/16 - 1s - loss: 1.8118 - accuracy: 0.3464 - val_loss: 1.8067 - val_accuracy: 0.3260 - 938ms/epoch - 59ms/step\n",
      "Epoch 38/100\n",
      "16/16 - 1s - loss: 1.7789 - accuracy: 0.3517 - val_loss: 1.8337 - val_accuracy: 0.3380 - 957ms/epoch - 60ms/step\n",
      "Epoch 39/100\n",
      "16/16 - 1s - loss: 1.7398 - accuracy: 0.3704 - val_loss: 1.8752 - val_accuracy: 0.3330 - 942ms/epoch - 59ms/step\n",
      "Epoch 40/100\n",
      "16/16 - 1s - loss: 1.7861 - accuracy: 0.3484 - val_loss: 1.8287 - val_accuracy: 0.3165 - 956ms/epoch - 60ms/step\n",
      "Epoch 41/100\n",
      "16/16 - 1s - loss: 1.7140 - accuracy: 0.3840 - val_loss: 1.7947 - val_accuracy: 0.3440 - 967ms/epoch - 60ms/step\n",
      "Epoch 42/100\n",
      "16/16 - 1s - loss: 1.7127 - accuracy: 0.3680 - val_loss: 1.8776 - val_accuracy: 0.3075 - 975ms/epoch - 61ms/step\n",
      "Epoch 43/100\n",
      "16/16 - 1s - loss: 1.7537 - accuracy: 0.3611 - val_loss: 1.8821 - val_accuracy: 0.3180 - 941ms/epoch - 59ms/step\n",
      "Epoch 44/100\n",
      "16/16 - 1s - loss: 1.7100 - accuracy: 0.3766 - val_loss: 1.7942 - val_accuracy: 0.3495 - 937ms/epoch - 59ms/step\n",
      "Epoch 45/100\n",
      "16/16 - 1s - loss: 1.7005 - accuracy: 0.3744 - val_loss: 1.7926 - val_accuracy: 0.3540 - 978ms/epoch - 61ms/step\n",
      "Epoch 46/100\n",
      "16/16 - 1s - loss: 1.7483 - accuracy: 0.3620 - val_loss: 1.7358 - val_accuracy: 0.3670 - 977ms/epoch - 61ms/step\n",
      "Epoch 47/100\n",
      "16/16 - 1s - loss: 1.6226 - accuracy: 0.4090 - val_loss: 1.8669 - val_accuracy: 0.3185 - 943ms/epoch - 59ms/step\n",
      "Epoch 48/100\n",
      "16/16 - 1s - loss: 1.7240 - accuracy: 0.3713 - val_loss: 1.8014 - val_accuracy: 0.3530 - 941ms/epoch - 59ms/step\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 1.6590 - accuracy: 0.4020 - val_loss: 1.7935 - val_accuracy: 0.3520 - 959ms/epoch - 60ms/step\n",
      "Epoch 50/100\n",
      "16/16 - 1s - loss: 1.6492 - accuracy: 0.3920 - val_loss: 1.8479 - val_accuracy: 0.3555 - 956ms/epoch - 60ms/step\n",
      "Epoch 51/100\n",
      "16/16 - 1s - loss: 1.6087 - accuracy: 0.4120 - val_loss: 1.8985 - val_accuracy: 0.3265 - 950ms/epoch - 59ms/step\n",
      "Epoch 52/100\n",
      "16/16 - 1s - loss: 1.6190 - accuracy: 0.4105 - val_loss: 1.8803 - val_accuracy: 0.3095 - 932ms/epoch - 58ms/step\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 1.6581 - accuracy: 0.3966 - val_loss: 1.8750 - val_accuracy: 0.3340 - 933ms/epoch - 58ms/step\n",
      "Epoch 54/100\n",
      "16/16 - 1s - loss: 1.6396 - accuracy: 0.4033 - val_loss: 1.7397 - val_accuracy: 0.3770 - 939ms/epoch - 59ms/step\n",
      "Epoch 55/100\n",
      "16/16 - 1s - loss: 1.5687 - accuracy: 0.4335 - val_loss: 1.7468 - val_accuracy: 0.3695 - 936ms/epoch - 58ms/step\n",
      "Epoch 56/100\n",
      "16/16 - 1s - loss: 1.5479 - accuracy: 0.4342 - val_loss: 1.9078 - val_accuracy: 0.3520 - 943ms/epoch - 59ms/step\n",
      "Epoch 57/100\n",
      "16/16 - 1s - loss: 1.5929 - accuracy: 0.4244 - val_loss: 1.8561 - val_accuracy: 0.3365 - 979ms/epoch - 61ms/step\n",
      "Epoch 58/100\n",
      "16/16 - 1s - loss: 1.5509 - accuracy: 0.4301 - val_loss: 1.7988 - val_accuracy: 0.3625 - 962ms/epoch - 60ms/step\n",
      "Epoch 59/100\n",
      "16/16 - 1s - loss: 1.5393 - accuracy: 0.4329 - val_loss: 1.8166 - val_accuracy: 0.3765 - 956ms/epoch - 60ms/step\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 1.4859 - accuracy: 0.4624 - val_loss: 1.8733 - val_accuracy: 0.3565 - 958ms/epoch - 60ms/step\n",
      "Epoch 61/100\n",
      "16/16 - 1s - loss: 1.5024 - accuracy: 0.4471 - val_loss: 1.7865 - val_accuracy: 0.3720 - 966ms/epoch - 60ms/step\n",
      "Epoch 62/100\n",
      "16/16 - 1s - loss: 1.4851 - accuracy: 0.4481 - val_loss: 1.8115 - val_accuracy: 0.3735 - 943ms/epoch - 59ms/step\n",
      "Epoch 63/100\n",
      "16/16 - 1s - loss: 1.5537 - accuracy: 0.4367 - val_loss: 1.7320 - val_accuracy: 0.3815 - 956ms/epoch - 60ms/step\n",
      "Epoch 64/100\n",
      "16/16 - 1s - loss: 1.4123 - accuracy: 0.4796 - val_loss: 1.7818 - val_accuracy: 0.3920 - 990ms/epoch - 62ms/step\n",
      "Epoch 65/100\n",
      "16/16 - 1s - loss: 1.4765 - accuracy: 0.4549 - val_loss: 1.8111 - val_accuracy: 0.3660 - 999ms/epoch - 62ms/step\n",
      "Epoch 66/100\n",
      "16/16 - 1s - loss: 1.4228 - accuracy: 0.4840 - val_loss: 1.8032 - val_accuracy: 0.3735 - 976ms/epoch - 61ms/step\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 1.4513 - accuracy: 0.4717 - val_loss: 1.9464 - val_accuracy: 0.3365 - 954ms/epoch - 60ms/step\n",
      "Epoch 68/100\n",
      "16/16 - 1s - loss: 1.3905 - accuracy: 0.4969 - val_loss: 1.9759 - val_accuracy: 0.3320 - 958ms/epoch - 60ms/step\n",
      "Epoch 69/100\n",
      "16/16 - 1s - loss: 1.4401 - accuracy: 0.4826 - val_loss: 1.7727 - val_accuracy: 0.3900 - 943ms/epoch - 59ms/step\n",
      "Epoch 70/100\n",
      "16/16 - 1s - loss: 1.3881 - accuracy: 0.4891 - val_loss: 1.8270 - val_accuracy: 0.3530 - 974ms/epoch - 61ms/step\n",
      "Epoch 71/100\n",
      "16/16 - 1s - loss: 1.3544 - accuracy: 0.5067 - val_loss: 1.8737 - val_accuracy: 0.3690 - 969ms/epoch - 61ms/step\n",
      "Epoch 72/100\n",
      "16/16 - 1s - loss: 1.3105 - accuracy: 0.5254 - val_loss: 2.0142 - val_accuracy: 0.3370 - 973ms/epoch - 61ms/step\n",
      "Epoch 73/100\n",
      "16/16 - 1s - loss: 1.4048 - accuracy: 0.4876 - val_loss: 2.0138 - val_accuracy: 0.3495 - 1s/epoch - 64ms/step\n",
      "Epoch 74/100\n",
      "16/16 - 1s - loss: 1.3904 - accuracy: 0.4960 - val_loss: 1.8702 - val_accuracy: 0.3590 - 977ms/epoch - 61ms/step\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 1.3320 - accuracy: 0.5120 - val_loss: 1.9747 - val_accuracy: 0.3675 - 946ms/epoch - 59ms/step\n",
      "Epoch 76/100\n",
      "16/16 - 1s - loss: 1.3591 - accuracy: 0.5115 - val_loss: 1.7654 - val_accuracy: 0.4030 - 971ms/epoch - 61ms/step\n",
      "Epoch 77/100\n",
      "16/16 - 1s - loss: 1.2730 - accuracy: 0.5378 - val_loss: 1.8517 - val_accuracy: 0.3745 - 957ms/epoch - 60ms/step\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 1.3225 - accuracy: 0.5322 - val_loss: 1.9017 - val_accuracy: 0.3440 - 962ms/epoch - 60ms/step\n",
      "Epoch 79/100\n",
      "16/16 - 1s - loss: 1.2247 - accuracy: 0.5586 - val_loss: 1.8631 - val_accuracy: 0.3625 - 939ms/epoch - 59ms/step\n",
      "Epoch 80/100\n",
      "16/16 - 1s - loss: 1.2205 - accuracy: 0.5551 - val_loss: 1.8504 - val_accuracy: 0.3820 - 956ms/epoch - 60ms/step\n",
      "Epoch 81/100\n",
      "16/16 - 1s - loss: 1.2183 - accuracy: 0.5585 - val_loss: 1.8424 - val_accuracy: 0.3940 - 954ms/epoch - 60ms/step\n",
      "Epoch 82/100\n",
      "16/16 - 1s - loss: 1.1914 - accuracy: 0.5707 - val_loss: 1.9420 - val_accuracy: 0.3780 - 945ms/epoch - 59ms/step\n",
      "Epoch 83/100\n",
      "16/16 - 1s - loss: 1.2036 - accuracy: 0.5685 - val_loss: 1.8535 - val_accuracy: 0.3965 - 957ms/epoch - 60ms/step\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.8897 - accuracy: 0.3899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8897080421447754, 0.38989999890327454]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class MakeModel(keras.models.Sequential):\n",
    "    def __init__(self, input_num, hidden_nums, output_num, **kwargs):\n",
    "        super().__init__([keras.layers.Input(shape=input_num),\n",
    "                          keras.layers.Flatten(input_shape=[32, 32, 3])] +\n",
    "                          [keras.layers.Dense(hidden_num, 'elu', kernel_initializer='he_normal') \n",
    "                           for hidden_num in hidden_nums] +\n",
    "                           [keras.layers.Dense(10, 'softmax')]\n",
    "                           )\n",
    "        \n",
    "        self.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "    \n",
    "h_ls = np.linspace(1028, 100, 20).astype(int).tolist()\n",
    "model = MakeModel((32, 32, 3), h_ls, 10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_valid, y_valid), verbose=2, \n",
    "                    callbacks=[early_stopping_cb, keras.callbacks.TensorBoard(get_run_logdir(1))])\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 12s - loss: 2.1578 - accuracy: 0.2738 - val_loss: 21.7483 - val_accuracy: 0.1050 - 12s/epoch - 766ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 1.6291 - accuracy: 0.4191 - val_loss: 14.1378 - val_accuracy: 0.1420 - 1s/epoch - 83ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 1.4607 - accuracy: 0.4864 - val_loss: 8.3764 - val_accuracy: 0.1370 - 1s/epoch - 89ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 2s - loss: 1.3215 - accuracy: 0.5385 - val_loss: 7.0054 - val_accuracy: 0.1625 - 2s/epoch - 129ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 2s - loss: 1.2462 - accuracy: 0.5616 - val_loss: 4.4123 - val_accuracy: 0.1960 - 2s/epoch - 103ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 1.0885 - accuracy: 0.6200 - val_loss: 3.8427 - val_accuracy: 0.2075 - 1s/epoch - 88ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 1.0134 - accuracy: 0.6436 - val_loss: 4.2179 - val_accuracy: 0.1650 - 1s/epoch - 85ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 0.9497 - accuracy: 0.6693 - val_loss: 4.3009 - val_accuracy: 0.2160 - 1s/epoch - 85ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 0.8630 - accuracy: 0.7055 - val_loss: 4.5788 - val_accuracy: 0.1830 - 1s/epoch - 88ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 0.7271 - accuracy: 0.7486 - val_loss: 3.1393 - val_accuracy: 0.3435 - 1s/epoch - 88ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 0.5447 - accuracy: 0.8140 - val_loss: 5.4030 - val_accuracy: 0.2005 - 1s/epoch - 85ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 0.6084 - accuracy: 0.7811 - val_loss: 4.8991 - val_accuracy: 0.2090 - 1s/epoch - 86ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 0.4789 - accuracy: 0.8369 - val_loss: 3.8430 - val_accuracy: 0.3045 - 1s/epoch - 86ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 2s - loss: 0.3763 - accuracy: 0.8729 - val_loss: 4.6719 - val_accuracy: 0.2580 - 2s/epoch - 95ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 2s - loss: 0.5355 - accuracy: 0.8154 - val_loss: 4.6033 - val_accuracy: 0.2500 - 2s/epoch - 95ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.2506 - accuracy: 0.9183 - val_loss: 4.0395 - val_accuracy: 0.3320 - 1s/epoch - 86ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.3021 - accuracy: 0.9010 - val_loss: 4.6556 - val_accuracy: 0.2480 - 1s/epoch - 93ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.3224 - accuracy: 0.8940 - val_loss: 4.3976 - val_accuracy: 0.2705 - 1s/epoch - 93ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 2s - loss: 0.2154 - accuracy: 0.9311 - val_loss: 6.0971 - val_accuracy: 0.2295 - 2s/epoch - 97ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.2237 - accuracy: 0.9250 - val_loss: 4.2254 - val_accuracy: 0.2730 - 1s/epoch - 92ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.3508 - accuracy: 0.8810 - val_loss: 4.5319 - val_accuracy: 0.2195 - 1s/epoch - 85ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.4796 - accuracy: 0.8468 - val_loss: 4.2164 - val_accuracy: 0.2890 - 1s/epoch - 89ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.1108 - accuracy: 0.9663 - val_loss: 3.8669 - val_accuracy: 0.3475 - 1s/epoch - 85ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.0737 - accuracy: 0.9800 - val_loss: 3.6210 - val_accuracy: 0.3905 - 1s/epoch - 88ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 2s - loss: 0.0741 - accuracy: 0.9781 - val_loss: 4.3554 - val_accuracy: 0.3495 - 2s/epoch - 95ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 1s - loss: 0.0920 - accuracy: 0.9700 - val_loss: 3.9886 - val_accuracy: 0.3470 - 1s/epoch - 88ms/step\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 0.2310 - accuracy: 0.9286 - val_loss: 4.1334 - val_accuracy: 0.3045 - 1s/epoch - 87ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 1s - loss: 0.4237 - accuracy: 0.8654 - val_loss: 4.0120 - val_accuracy: 0.3115 - 1s/epoch - 88ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 1s - loss: 0.0880 - accuracy: 0.9754 - val_loss: 3.5222 - val_accuracy: 0.3665 - 1s/epoch - 90ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 2s - loss: 0.0430 - accuracy: 0.9886 - val_loss: 4.1291 - val_accuracy: 0.3430 - 2s/epoch - 95ms/step\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 4.1042 - accuracy: 0.3362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.104167461395264, 0.3361999988555908]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MakeModel(keras.models.Model):\n",
    "    def __init__(self, input_num, hidden_nums, output_num, **kwargs):\n",
    "        x = keras.layers.Input(shape=input_num)\n",
    "        h = keras.layers.Flatten(input_shape=[32, 32, 3])(x)\n",
    "        for hidden_num in hidden_nums:\n",
    "            h = keras.layers.Dense(hidden_num, 'elu', kernel_initializer='he_normal')(h)\n",
    "            h = keras.layers.BatchNormalization()(h)\n",
    "        y = keras.layers.Dense(10, 'softmax')(h)\n",
    "        super().__init__(x, y)\n",
    "        self.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "    \n",
    "h_ls = np.linspace(1028, 100, 20).astype(int).tolist()\n",
    "model = MakeModel((32, 32, 3), h_ls, 10)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_valid, y_valid), verbose=2, \n",
    "                    callbacks=[early_stopping_cb, keras.callbacks.TensorBoard(get_run_logdir(2))])\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 11s - loss: 2.2154 - accuracy: 0.2463 - val_loss: 3.6938 - val_accuracy: 0.2180 - 11s/epoch - 718ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 1.7900 - accuracy: 0.3569 - val_loss: 3.3271 - val_accuracy: 0.2375 - 1s/epoch - 83ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 1s - loss: 1.6747 - accuracy: 0.4027 - val_loss: 3.1751 - val_accuracy: 0.2545 - 1s/epoch - 87ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 1.5879 - accuracy: 0.4386 - val_loss: 3.1795 - val_accuracy: 0.2830 - 1s/epoch - 90ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 2s - loss: 1.5006 - accuracy: 0.4706 - val_loss: 2.5543 - val_accuracy: 0.3400 - 2s/epoch - 118ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 1.4697 - accuracy: 0.4839 - val_loss: 2.3632 - val_accuracy: 0.3595 - 1s/epoch - 92ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 1.4146 - accuracy: 0.5058 - val_loss: 3.8070 - val_accuracy: 0.2750 - 1s/epoch - 88ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 1s - loss: 1.3799 - accuracy: 0.5231 - val_loss: 3.0113 - val_accuracy: 0.2670 - 1s/epoch - 86ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 1s - loss: 1.2532 - accuracy: 0.5614 - val_loss: 2.7014 - val_accuracy: 0.3275 - 1s/epoch - 91ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 1s - loss: 1.2308 - accuracy: 0.5690 - val_loss: 2.8948 - val_accuracy: 0.3430 - 1s/epoch - 86ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 1s - loss: 1.1891 - accuracy: 0.5896 - val_loss: 3.9202 - val_accuracy: 0.2595 - 1s/epoch - 88ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 1s - loss: 1.1571 - accuracy: 0.5947 - val_loss: 2.7745 - val_accuracy: 0.3465 - 1s/epoch - 86ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 1.0856 - accuracy: 0.6198 - val_loss: 2.6990 - val_accuracy: 0.3620 - 1s/epoch - 86ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 1s - loss: 1.0503 - accuracy: 0.6308 - val_loss: 3.0479 - val_accuracy: 0.3100 - 1s/epoch - 84ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 0.9681 - accuracy: 0.6679 - val_loss: 3.0620 - val_accuracy: 0.3425 - 1s/epoch - 92ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 0.9216 - accuracy: 0.6774 - val_loss: 2.7548 - val_accuracy: 0.3560 - 1s/epoch - 91ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 1s - loss: 0.8610 - accuracy: 0.7029 - val_loss: 3.4274 - val_accuracy: 0.3135 - 1s/epoch - 89ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 1s - loss: 0.8357 - accuracy: 0.7101 - val_loss: 2.7785 - val_accuracy: 0.3720 - 1s/epoch - 86ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 1s - loss: 0.7435 - accuracy: 0.7465 - val_loss: 3.2941 - val_accuracy: 0.3635 - 1s/epoch - 86ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 1s - loss: 0.7264 - accuracy: 0.7542 - val_loss: 2.9078 - val_accuracy: 0.3745 - 1s/epoch - 87ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 1s - loss: 0.6184 - accuracy: 0.7896 - val_loss: 3.5492 - val_accuracy: 0.3245 - 1s/epoch - 89ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 1s - loss: 0.7334 - accuracy: 0.7490 - val_loss: 3.0620 - val_accuracy: 0.3425 - 1s/epoch - 89ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 1s - loss: 0.5430 - accuracy: 0.8191 - val_loss: 3.1105 - val_accuracy: 0.3535 - 1s/epoch - 90ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.5548 - accuracy: 0.8056 - val_loss: 3.5959 - val_accuracy: 0.3195 - 1s/epoch - 87ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.4974 - accuracy: 0.8322 - val_loss: 3.4459 - val_accuracy: 0.3505 - 1s/epoch - 91ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 2s - loss: 0.4752 - accuracy: 0.8372 - val_loss: 3.7727 - val_accuracy: 0.3315 - 2s/epoch - 94ms/step\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 3.7842 - accuracy: 0.3432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.7841546535491943, 0.3431999981403351]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_means = X_train_full[2000:10000].mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train_full[2000:10000].std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train_full[2000:10000] - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_train_full[:2000] - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test_full - pixel_means) / pixel_stds\n",
    "\n",
    "x = keras.layers.Input(shape=X_train.shape[1:])\n",
    "h = keras.layers.Flatten(input_shape=X_train.shape[1:])(x)\n",
    "for hidden_num in h_ls:\n",
    "    h = keras.layers.Dense(hidden_num, 'selu', kernel_initializer='lecun_normal')(h)\n",
    "    h = keras.layers.BatchNormalization()(h)\n",
    "y = keras.layers.Dense(10, 'softmax')(h)\n",
    "model = keras.Model(x, y)\n",
    "model.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=512, validation_data=(X_valid_scaled, y_valid), verbose=2, \n",
    "                    callbacks=[early_stopping_cb, keras.callbacks.TensorBoard(get_run_logdir(3))])\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 13s - loss: 3.0184 - accuracy: 0.1015 - val_loss: 2.8632 - val_accuracy: 0.1220 - 13s/epoch - 811ms/step\n",
      "Epoch 2/100\n",
      "16/16 - 1s - loss: 2.9525 - accuracy: 0.1000 - val_loss: 2.7808 - val_accuracy: 0.1145 - 1s/epoch - 91ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 2s - loss: 2.8248 - accuracy: 0.1046 - val_loss: 2.8957 - val_accuracy: 0.1065 - 2s/epoch - 99ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 1s - loss: 2.7818 - accuracy: 0.0978 - val_loss: 2.6763 - val_accuracy: 0.1205 - 1s/epoch - 93ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 1s - loss: 2.7047 - accuracy: 0.0988 - val_loss: 2.6166 - val_accuracy: 0.1120 - 1s/epoch - 90ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 1s - loss: 2.6429 - accuracy: 0.1005 - val_loss: 2.4326 - val_accuracy: 0.1220 - 1s/epoch - 93ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 1s - loss: 2.6034 - accuracy: 0.0945 - val_loss: 2.4396 - val_accuracy: 0.0990 - 1s/epoch - 94ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 2s - loss: 2.5527 - accuracy: 0.1046 - val_loss: 2.3925 - val_accuracy: 0.1005 - 2s/epoch - 94ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 2s - loss: 2.5293 - accuracy: 0.0989 - val_loss: 2.3932 - val_accuracy: 0.1045 - 2s/epoch - 126ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 2s - loss: 2.4882 - accuracy: 0.1026 - val_loss: 2.3787 - val_accuracy: 0.0920 - 2s/epoch - 102ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 2s - loss: 2.4747 - accuracy: 0.1010 - val_loss: 2.3553 - val_accuracy: 0.0895 - 2s/epoch - 96ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 2s - loss: 2.4504 - accuracy: 0.1032 - val_loss: 2.3471 - val_accuracy: 0.0945 - 2s/epoch - 94ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 1s - loss: 2.4311 - accuracy: 0.1000 - val_loss: 2.3230 - val_accuracy: 0.0935 - 1s/epoch - 92ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 2s - loss: 2.4097 - accuracy: 0.1015 - val_loss: 2.2851 - val_accuracy: 0.1120 - 2s/epoch - 96ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 1s - loss: 2.4016 - accuracy: 0.1015 - val_loss: 2.2893 - val_accuracy: 0.1305 - 1s/epoch - 92ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 1s - loss: 2.3826 - accuracy: 0.0945 - val_loss: 2.3101 - val_accuracy: 0.1155 - 1s/epoch - 93ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 2s - loss: 2.3687 - accuracy: 0.1016 - val_loss: 2.3208 - val_accuracy: 0.1095 - 2s/epoch - 99ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 2s - loss: 2.3639 - accuracy: 0.0994 - val_loss: 2.3087 - val_accuracy: 0.1080 - 2s/epoch - 99ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 2s - loss: 2.3591 - accuracy: 0.0967 - val_loss: 2.2985 - val_accuracy: 0.1075 - 2s/epoch - 98ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 2s - loss: 2.3526 - accuracy: 0.1019 - val_loss: 2.2874 - val_accuracy: 0.1495 - 2s/epoch - 95ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 2s - loss: 2.3450 - accuracy: 0.0986 - val_loss: 2.2890 - val_accuracy: 0.1405 - 2s/epoch - 96ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 2s - loss: 2.3391 - accuracy: 0.0980 - val_loss: 2.2853 - val_accuracy: 0.1240 - 2s/epoch - 95ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 2s - loss: 2.3327 - accuracy: 0.1009 - val_loss: 2.2864 - val_accuracy: 0.1335 - 2s/epoch - 101ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 2s - loss: 2.3260 - accuracy: 0.1037 - val_loss: 2.2983 - val_accuracy: 0.1380 - 2s/epoch - 96ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 2.3274 - accuracy: 0.1019 - val_loss: 2.2886 - val_accuracy: 0.1440 - 1s/epoch - 94ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 2s - loss: 2.3232 - accuracy: 0.1031 - val_loss: 2.3022 - val_accuracy: 0.1115 - 2s/epoch - 94ms/step\n",
      "Epoch 27/100\n",
      "16/16 - 1s - loss: 2.3206 - accuracy: 0.1024 - val_loss: 2.3008 - val_accuracy: 0.1265 - 1s/epoch - 92ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 2s - loss: 2.3156 - accuracy: 0.1005 - val_loss: 2.3043 - val_accuracy: 0.1315 - 2s/epoch - 99ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 2s - loss: 2.3148 - accuracy: 0.1072 - val_loss: 2.3044 - val_accuracy: 0.1330 - 2s/epoch - 99ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 2s - loss: 2.3145 - accuracy: 0.1021 - val_loss: 2.3053 - val_accuracy: 0.1285 - 2s/epoch - 94ms/step\n",
      "Epoch 31/100\n",
      "16/16 - 2s - loss: 2.3131 - accuracy: 0.1034 - val_loss: 2.3071 - val_accuracy: 0.1205 - 2s/epoch - 100ms/step\n",
      "Epoch 32/100\n",
      "16/16 - 2s - loss: 2.3097 - accuracy: 0.1061 - val_loss: 2.3070 - val_accuracy: 0.1155 - 2s/epoch - 94ms/step\n",
      "Epoch 33/100\n",
      "16/16 - 2s - loss: 2.3125 - accuracy: 0.0954 - val_loss: 2.3108 - val_accuracy: 0.1230 - 2s/epoch - 96ms/step\n",
      "Epoch 34/100\n",
      "16/16 - 1s - loss: 2.3096 - accuracy: 0.1023 - val_loss: 2.3138 - val_accuracy: 0.0975 - 1s/epoch - 94ms/step\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 2.3130 - accuracy: 0.0945\n",
      "[2.313009023666382, 0.09449999779462814]\n",
      "313/313 [==============================] - 5s 14ms/step\n",
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 4s 13ms/step\n",
      "313/313 [==============================] - 4s 13ms/step\n",
      "313/313 [==============================] - 5s 16ms/step\n",
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 5s 15ms/step\n",
      "313/313 [==============================] - 4s 14ms/step\n",
      "313/313 [==============================] - 4s 13ms/step\n",
      "313/313 [==============================] - 4s 12ms/step\n",
      "313/313 [==============================] - 4s 13ms/step\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m mc_model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m mc_model\u001b[38;5;241m.\u001b[39mset_weights(model\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m---> 32\u001b[0m y_probas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([mc_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     33\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)])\n\u001b[0;32m     35\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m y_probas\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m mc_model\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m mc_model\u001b[38;5;241m.\u001b[39mset_weights(model\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m---> 32\u001b[0m y_probas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\u001b[43mmc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)])\n\u001b[0;32m     35\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m y_probas\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\why\\miniforge3\\envs\\study\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\why\\miniforge3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "class MakeModel(keras.models.Sequential):\n",
    "    def __init__(self, input_num, hidden_nums, output_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.add(keras.layers.Input(shape=input_num))\n",
    "        self.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "        for hidden_num in hidden_nums:\n",
    "            self.add(keras.layers.Dense(hidden_num, 'selu', kernel_initializer='lecun_normal'))\n",
    "            self.add(keras.layers.BatchNormalization())\n",
    "            self.add(keras.layers.AlphaDropout(0.5))\n",
    "        self.add(keras.layers.Dense(10, 'softmax'))\n",
    "        self.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "model = MakeModel((32, 32, 3), h_ls, 10)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=512, validation_data=(X_valid_scaled, y_valid), verbose=2, \n",
    "                    callbacks=[early_stopping_cb, keras.callbacks.TensorBoard(get_run_logdir(4))])\n",
    "print(model.evaluate(X_test_scaled, y_test))\n",
    "\n",
    "class MCDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "y_probas = np.stack([mc_model.predict(X_test_scaled)\n",
    "                     for sample in range(100)])\n",
    "                     \n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 547s 68ms/step - loss: 2781.1404 - accuracy: 0.1013\n",
      "9.676056\n"
     ]
    }
   ],
   "source": [
    "# 최적 학습률 확인\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(model.optimizer.lr.numpy())\n",
    "        self.losses.append(batch_loss)\n",
    "        self.model.optimizer.lr = self.model.optimizer.lr * self.factor\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = model.optimizer.lr.numpy()\n",
    "    model.optimizer.lr = min_rate\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    model.optimizer.lr = init_lr\n",
    "    model.set_weights(init_weights)\n",
    "\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "model = MakeModel((32, 32, 3), h_ls, 10)\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=1)\n",
    "lr = rates[losses.index(min(losses))-10]\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 - 20s - loss: 2.2121 - accuracy: 0.2447 - val_loss: 3.4649 - val_accuracy: 0.2500 - 20s/epoch - 1s/step\n",
      "Epoch 2/100\n",
      "16/16 - 3s - loss: 1.7829 - accuracy: 0.3664 - val_loss: 3.0001 - val_accuracy: 0.2910 - 3s/epoch - 175ms/step\n",
      "Epoch 3/100\n",
      "16/16 - 2s - loss: 1.6860 - accuracy: 0.3963 - val_loss: 2.8616 - val_accuracy: 0.3155 - 2s/epoch - 117ms/step\n",
      "Epoch 4/100\n",
      "16/16 - 2s - loss: 1.5879 - accuracy: 0.4366 - val_loss: 2.8086 - val_accuracy: 0.3165 - 2s/epoch - 95ms/step\n",
      "Epoch 5/100\n",
      "16/16 - 2s - loss: 1.5478 - accuracy: 0.4615 - val_loss: 2.8607 - val_accuracy: 0.3380 - 2s/epoch - 116ms/step\n",
      "Epoch 6/100\n",
      "16/16 - 2s - loss: 1.5059 - accuracy: 0.4723 - val_loss: 2.8439 - val_accuracy: 0.3260 - 2s/epoch - 128ms/step\n",
      "Epoch 7/100\n",
      "16/16 - 3s - loss: 1.3857 - accuracy: 0.5159 - val_loss: 2.7435 - val_accuracy: 0.3285 - 3s/epoch - 184ms/step\n",
      "Epoch 8/100\n",
      "16/16 - 3s - loss: 1.3865 - accuracy: 0.5163 - val_loss: 2.6510 - val_accuracy: 0.3680 - 3s/epoch - 170ms/step\n",
      "Epoch 9/100\n",
      "16/16 - 3s - loss: 1.2910 - accuracy: 0.5474 - val_loss: 3.1025 - val_accuracy: 0.3125 - 3s/epoch - 156ms/step\n",
      "Epoch 10/100\n",
      "16/16 - 2s - loss: 1.2317 - accuracy: 0.5717 - val_loss: 3.1800 - val_accuracy: 0.3275 - 2s/epoch - 119ms/step\n",
      "Epoch 11/100\n",
      "16/16 - 2s - loss: 1.1970 - accuracy: 0.5893 - val_loss: 3.7023 - val_accuracy: 0.2640 - 2s/epoch - 117ms/step\n",
      "Epoch 12/100\n",
      "16/16 - 2s - loss: 1.1988 - accuracy: 0.5872 - val_loss: 2.7806 - val_accuracy: 0.3605 - 2s/epoch - 96ms/step\n",
      "Epoch 13/100\n",
      "16/16 - 2s - loss: 1.0790 - accuracy: 0.6237 - val_loss: 3.3146 - val_accuracy: 0.3295 - 2s/epoch - 142ms/step\n",
      "Epoch 14/100\n",
      "16/16 - 3s - loss: 1.0605 - accuracy: 0.6301 - val_loss: 2.9379 - val_accuracy: 0.3565 - 3s/epoch - 157ms/step\n",
      "Epoch 15/100\n",
      "16/16 - 2s - loss: 0.9748 - accuracy: 0.6597 - val_loss: 3.2334 - val_accuracy: 0.3110 - 2s/epoch - 108ms/step\n",
      "Epoch 16/100\n",
      "16/16 - 3s - loss: 0.9447 - accuracy: 0.6714 - val_loss: 2.6515 - val_accuracy: 0.3745 - 3s/epoch - 165ms/step\n",
      "Epoch 17/100\n",
      "16/16 - 3s - loss: 0.8835 - accuracy: 0.6902 - val_loss: 2.4164 - val_accuracy: 0.4040 - 3s/epoch - 163ms/step\n",
      "Epoch 18/100\n",
      "16/16 - 2s - loss: 0.8087 - accuracy: 0.7240 - val_loss: 3.1230 - val_accuracy: 0.3415 - 2s/epoch - 117ms/step\n",
      "Epoch 19/100\n",
      "16/16 - 2s - loss: 0.7890 - accuracy: 0.7256 - val_loss: 3.2004 - val_accuracy: 0.3635 - 2s/epoch - 125ms/step\n",
      "Epoch 20/100\n",
      "16/16 - 2s - loss: 0.7669 - accuracy: 0.7346 - val_loss: 3.7813 - val_accuracy: 0.3335 - 2s/epoch - 121ms/step\n",
      "Epoch 21/100\n",
      "16/16 - 2s - loss: 0.6919 - accuracy: 0.7621 - val_loss: 3.4561 - val_accuracy: 0.3345 - 2s/epoch - 96ms/step\n",
      "Epoch 22/100\n",
      "16/16 - 2s - loss: 0.5439 - accuracy: 0.8167 - val_loss: 4.0070 - val_accuracy: 0.3220 - 2s/epoch - 104ms/step\n",
      "Epoch 23/100\n",
      "16/16 - 2s - loss: 0.7167 - accuracy: 0.7599 - val_loss: 2.9375 - val_accuracy: 0.3525 - 2s/epoch - 112ms/step\n",
      "Epoch 24/100\n",
      "16/16 - 1s - loss: 0.5674 - accuracy: 0.8105 - val_loss: 3.3179 - val_accuracy: 0.3445 - 1s/epoch - 93ms/step\n",
      "Epoch 25/100\n",
      "16/16 - 1s - loss: 0.5334 - accuracy: 0.8239 - val_loss: 2.8520 - val_accuracy: 0.4080 - 1s/epoch - 88ms/step\n",
      "Epoch 26/100\n",
      "16/16 - 2s - loss: 0.4599 - accuracy: 0.8471 - val_loss: 3.0699 - val_accuracy: 0.4050 - 2s/epoch - 99ms/step\n",
      "Epoch 27/100\n",
      "16/16 - 2s - loss: 0.4141 - accuracy: 0.8565 - val_loss: 3.3176 - val_accuracy: 0.3840 - 2s/epoch - 113ms/step\n",
      "Epoch 28/100\n",
      "16/16 - 2s - loss: 0.4419 - accuracy: 0.8537 - val_loss: 3.4924 - val_accuracy: 0.3365 - 2s/epoch - 96ms/step\n",
      "Epoch 29/100\n",
      "16/16 - 2s - loss: 0.4026 - accuracy: 0.8622 - val_loss: 3.4558 - val_accuracy: 0.3665 - 2s/epoch - 102ms/step\n",
      "Epoch 30/100\n",
      "16/16 - 2s - loss: 0.2857 - accuracy: 0.9062 - val_loss: 4.1097 - val_accuracy: 0.3570 - 2s/epoch - 111ms/step\n",
      "Epoch 31/100\n",
      "16/16 - 2s - loss: 0.4061 - accuracy: 0.8616 - val_loss: 4.1829 - val_accuracy: 0.3530 - 2s/epoch - 96ms/step\n",
      "Epoch 32/100\n",
      "16/16 - 1s - loss: 0.2503 - accuracy: 0.9190 - val_loss: 3.3313 - val_accuracy: 0.3995 - 1s/epoch - 90ms/step\n",
      "Epoch 33/100\n",
      "16/16 - 2s - loss: 0.2842 - accuracy: 0.9087 - val_loss: 3.8874 - val_accuracy: 0.3435 - 2s/epoch - 97ms/step\n",
      "Epoch 34/100\n",
      "16/16 - 2s - loss: 0.2967 - accuracy: 0.9060 - val_loss: 3.8431 - val_accuracy: 0.3690 - 2s/epoch - 113ms/step\n",
      "Epoch 35/100\n",
      "16/16 - 2s - loss: 0.1915 - accuracy: 0.9386 - val_loss: 4.1189 - val_accuracy: 0.3655 - 2s/epoch - 98ms/step\n",
      "Epoch 36/100\n",
      "16/16 - 1s - loss: 0.3380 - accuracy: 0.8961 - val_loss: 3.1532 - val_accuracy: 0.3960 - 1s/epoch - 93ms/step\n",
      "Epoch 37/100\n",
      "16/16 - 3s - loss: 0.0993 - accuracy: 0.9719 - val_loss: 3.7338 - val_accuracy: 0.3880 - 3s/epoch - 177ms/step\n",
      "Epoch 38/100\n",
      "16/16 - 2s - loss: 0.1591 - accuracy: 0.9521 - val_loss: 3.9252 - val_accuracy: 0.3590 - 2s/epoch - 153ms/step\n",
      "Epoch 39/100\n",
      "16/16 - 2s - loss: 0.3553 - accuracy: 0.8854 - val_loss: 4.2803 - val_accuracy: 0.2855 - 2s/epoch - 98ms/step\n",
      "Epoch 40/100\n",
      "16/16 - 2s - loss: 0.3081 - accuracy: 0.9069 - val_loss: 3.3126 - val_accuracy: 0.4095 - 2s/epoch - 127ms/step\n",
      "Epoch 41/100\n",
      "16/16 - 2s - loss: 0.0895 - accuracy: 0.9722 - val_loss: 3.4493 - val_accuracy: 0.4205 - 2s/epoch - 97ms/step\n",
      "Epoch 42/100\n",
      "16/16 - 2s - loss: 0.3197 - accuracy: 0.9004 - val_loss: 3.6986 - val_accuracy: 0.3390 - 2s/epoch - 97ms/step\n",
      "Epoch 43/100\n",
      "16/16 - 2s - loss: 0.1925 - accuracy: 0.9441 - val_loss: 3.1995 - val_accuracy: 0.4305 - 2s/epoch - 135ms/step\n",
      "Epoch 44/100\n",
      "16/16 - 2s - loss: 0.0660 - accuracy: 0.9821 - val_loss: 3.6957 - val_accuracy: 0.4070 - 2s/epoch - 129ms/step\n",
      "Epoch 45/100\n",
      "16/16 - 2s - loss: 0.0515 - accuracy: 0.9861 - val_loss: 3.6073 - val_accuracy: 0.4250 - 2s/epoch - 105ms/step\n",
      "Epoch 46/100\n",
      "16/16 - 2s - loss: 0.0623 - accuracy: 0.9814 - val_loss: 3.6389 - val_accuracy: 0.4325 - 2s/epoch - 106ms/step\n",
      "Epoch 47/100\n",
      "16/16 - 2s - loss: 0.1813 - accuracy: 0.9476 - val_loss: 4.0030 - val_accuracy: 0.3650 - 2s/epoch - 110ms/step\n",
      "Epoch 48/100\n",
      "16/16 - 2s - loss: 0.2663 - accuracy: 0.9143 - val_loss: 3.0509 - val_accuracy: 0.4365 - 2s/epoch - 103ms/step\n",
      "Epoch 49/100\n",
      "16/16 - 1s - loss: 0.0712 - accuracy: 0.9789 - val_loss: 3.5053 - val_accuracy: 0.4255 - 1s/epoch - 91ms/step\n",
      "Epoch 50/100\n",
      "16/16 - 2s - loss: 0.0847 - accuracy: 0.9740 - val_loss: 3.6799 - val_accuracy: 0.4290 - 2s/epoch - 111ms/step\n",
      "Epoch 51/100\n",
      "16/16 - 2s - loss: 0.1404 - accuracy: 0.9528 - val_loss: 3.2523 - val_accuracy: 0.4315 - 2s/epoch - 134ms/step\n",
      "Epoch 52/100\n",
      "16/16 - 2s - loss: 0.0433 - accuracy: 0.9875 - val_loss: 3.6193 - val_accuracy: 0.4270 - 2s/epoch - 97ms/step\n",
      "Epoch 53/100\n",
      "16/16 - 1s - loss: 0.0359 - accuracy: 0.9894 - val_loss: 4.3344 - val_accuracy: 0.3940 - 1s/epoch - 93ms/step\n",
      "Epoch 54/100\n",
      "16/16 - 2s - loss: 0.3957 - accuracy: 0.8865 - val_loss: 2.9642 - val_accuracy: 0.4195 - 2s/epoch - 127ms/step\n",
      "Epoch 55/100\n",
      "16/16 - 2s - loss: 0.0774 - accuracy: 0.9787 - val_loss: 3.4384 - val_accuracy: 0.4185 - 2s/epoch - 104ms/step\n",
      "Epoch 56/100\n",
      "16/16 - 2s - loss: 0.0438 - accuracy: 0.9870 - val_loss: 3.5240 - val_accuracy: 0.4420 - 2s/epoch - 95ms/step\n",
      "Epoch 57/100\n",
      "16/16 - 2s - loss: 0.0279 - accuracy: 0.9919 - val_loss: 4.2507 - val_accuracy: 0.3950 - 2s/epoch - 108ms/step\n",
      "Epoch 58/100\n",
      "16/16 - 2s - loss: 0.1634 - accuracy: 0.9528 - val_loss: 3.2928 - val_accuracy: 0.4530 - 2s/epoch - 104ms/step\n",
      "Epoch 59/100\n",
      "16/16 - 2s - loss: 0.0476 - accuracy: 0.9859 - val_loss: 3.6780 - val_accuracy: 0.4270 - 2s/epoch - 97ms/step\n",
      "Epoch 60/100\n",
      "16/16 - 1s - loss: 0.0391 - accuracy: 0.9889 - val_loss: 3.7472 - val_accuracy: 0.4240 - 1s/epoch - 94ms/step\n",
      "Epoch 61/100\n",
      "16/16 - 2s - loss: 0.1885 - accuracy: 0.9430 - val_loss: 3.6408 - val_accuracy: 0.3825 - 2s/epoch - 102ms/step\n",
      "Epoch 62/100\n",
      "16/16 - 2s - loss: 0.1391 - accuracy: 0.9586 - val_loss: 3.5558 - val_accuracy: 0.4160 - 2s/epoch - 101ms/step\n",
      "Epoch 63/100\n",
      "16/16 - 2s - loss: 0.0662 - accuracy: 0.9790 - val_loss: 3.7411 - val_accuracy: 0.3885 - 2s/epoch - 99ms/step\n",
      "Epoch 64/100\n",
      "16/16 - 2s - loss: 0.0557 - accuracy: 0.9831 - val_loss: 3.6934 - val_accuracy: 0.4120 - 2s/epoch - 95ms/step\n",
      "Epoch 65/100\n",
      "16/16 - 2s - loss: 0.0476 - accuracy: 0.9861 - val_loss: 3.5620 - val_accuracy: 0.4410 - 2s/epoch - 105ms/step\n",
      "Epoch 66/100\n",
      "16/16 - 2s - loss: 0.0236 - accuracy: 0.9934 - val_loss: 3.5448 - val_accuracy: 0.4455 - 2s/epoch - 101ms/step\n",
      "Epoch 67/100\n",
      "16/16 - 1s - loss: 0.0374 - accuracy: 0.9887 - val_loss: 3.6243 - val_accuracy: 0.4405 - 1s/epoch - 88ms/step\n",
      "Epoch 68/100\n",
      "16/16 - 2s - loss: 0.1150 - accuracy: 0.9644 - val_loss: 3.3056 - val_accuracy: 0.4385 - 2s/epoch - 94ms/step\n",
      "Epoch 69/100\n",
      "16/16 - 2s - loss: 0.0548 - accuracy: 0.9831 - val_loss: 3.4472 - val_accuracy: 0.4380 - 2s/epoch - 122ms/step\n",
      "Epoch 70/100\n",
      "16/16 - 2s - loss: 0.0423 - accuracy: 0.9876 - val_loss: 3.6581 - val_accuracy: 0.4390 - 2s/epoch - 98ms/step\n",
      "Epoch 71/100\n",
      "16/16 - 2s - loss: 0.1250 - accuracy: 0.9624 - val_loss: 4.2616 - val_accuracy: 0.3230 - 2s/epoch - 95ms/step\n",
      "Epoch 72/100\n",
      "16/16 - 2s - loss: 0.3301 - accuracy: 0.9095 - val_loss: 3.0505 - val_accuracy: 0.4310 - 2s/epoch - 98ms/step\n",
      "Epoch 73/100\n",
      "16/16 - 2s - loss: 0.0448 - accuracy: 0.9872 - val_loss: 3.3928 - val_accuracy: 0.4295 - 2s/epoch - 129ms/step\n",
      "Epoch 74/100\n",
      "16/16 - 2s - loss: 0.0215 - accuracy: 0.9951 - val_loss: 3.5128 - val_accuracy: 0.4560 - 2s/epoch - 96ms/step\n",
      "Epoch 75/100\n",
      "16/16 - 1s - loss: 0.0110 - accuracy: 0.9973 - val_loss: 3.5728 - val_accuracy: 0.4535 - 1s/epoch - 87ms/step\n",
      "Epoch 76/100\n",
      "16/16 - 2s - loss: 0.0054 - accuracy: 0.9990 - val_loss: 3.6310 - val_accuracy: 0.4525 - 2s/epoch - 102ms/step\n",
      "Epoch 77/100\n",
      "16/16 - 2s - loss: 0.0127 - accuracy: 0.9965 - val_loss: 3.9534 - val_accuracy: 0.4145 - 2s/epoch - 97ms/step\n",
      "Epoch 78/100\n",
      "16/16 - 1s - loss: 0.0389 - accuracy: 0.9887 - val_loss: 4.3538 - val_accuracy: 0.4075 - 1s/epoch - 90ms/step\n",
      "Epoch 79/100\n",
      "16/16 - 2s - loss: 0.0994 - accuracy: 0.9700 - val_loss: 3.5751 - val_accuracy: 0.4295 - 2s/epoch - 100ms/step\n",
      "Epoch 80/100\n",
      "16/16 - 2s - loss: 0.1540 - accuracy: 0.9521 - val_loss: 3.6071 - val_accuracy: 0.4105 - 2s/epoch - 118ms/step\n",
      "Epoch 81/100\n",
      "16/16 - 2s - loss: 0.0558 - accuracy: 0.9824 - val_loss: 3.5163 - val_accuracy: 0.4190 - 2s/epoch - 101ms/step\n",
      "Epoch 82/100\n",
      "16/16 - 2s - loss: 0.0516 - accuracy: 0.9841 - val_loss: 3.5313 - val_accuracy: 0.4230 - 2s/epoch - 99ms/step\n",
      "Epoch 83/100\n",
      "16/16 - 2s - loss: 0.0341 - accuracy: 0.9896 - val_loss: 3.8758 - val_accuracy: 0.4240 - 2s/epoch - 100ms/step\n",
      "Epoch 84/100\n",
      "16/16 - 2s - loss: 0.0669 - accuracy: 0.9780 - val_loss: 3.8730 - val_accuracy: 0.4195 - 2s/epoch - 102ms/step\n",
      "Epoch 85/100\n",
      "16/16 - 2s - loss: 0.1224 - accuracy: 0.9634 - val_loss: 3.6375 - val_accuracy: 0.4155 - 2s/epoch - 98ms/step\n",
      "Epoch 86/100\n",
      "16/16 - 2s - loss: 0.0584 - accuracy: 0.9835 - val_loss: 3.4017 - val_accuracy: 0.4420 - 2s/epoch - 96ms/step\n",
      "Epoch 87/100\n",
      "16/16 - 2s - loss: 0.0306 - accuracy: 0.9905 - val_loss: 3.5757 - val_accuracy: 0.4385 - 2s/epoch - 107ms/step\n",
      "Epoch 88/100\n",
      "16/16 - 2s - loss: 0.0331 - accuracy: 0.9880 - val_loss: 3.8463 - val_accuracy: 0.4135 - 2s/epoch - 105ms/step\n",
      "Epoch 89/100\n",
      "16/16 - 2s - loss: 0.0432 - accuracy: 0.9869 - val_loss: 3.7156 - val_accuracy: 0.4300 - 2s/epoch - 94ms/step\n",
      "Epoch 90/100\n",
      "16/16 - 2s - loss: 0.0939 - accuracy: 0.9728 - val_loss: 3.5803 - val_accuracy: 0.4290 - 2s/epoch - 101ms/step\n",
      "Epoch 91/100\n",
      "16/16 - 2s - loss: 0.0473 - accuracy: 0.9856 - val_loss: 3.8290 - val_accuracy: 0.4325 - 2s/epoch - 131ms/step\n",
      "Epoch 92/100\n",
      "16/16 - 2s - loss: 0.0601 - accuracy: 0.9812 - val_loss: 3.6111 - val_accuracy: 0.4495 - 2s/epoch - 99ms/step\n",
      "Epoch 93/100\n",
      "16/16 - 1s - loss: 0.0473 - accuracy: 0.9850 - val_loss: 3.6869 - val_accuracy: 0.4295 - 1s/epoch - 92ms/step\n",
      "Epoch 94/100\n",
      "16/16 - 2s - loss: 0.0504 - accuracy: 0.9840 - val_loss: 4.3472 - val_accuracy: 0.4130 - 2s/epoch - 103ms/step\n",
      "Epoch 95/100\n",
      "16/16 - 2s - loss: 0.0811 - accuracy: 0.9751 - val_loss: 3.6018 - val_accuracy: 0.4210 - 2s/epoch - 100ms/step\n",
      "Epoch 96/100\n",
      "16/16 - 1s - loss: 0.0339 - accuracy: 0.9901 - val_loss: 3.7244 - val_accuracy: 0.4405 - 1s/epoch - 90ms/step\n",
      "Epoch 97/100\n",
      "16/16 - 1s - loss: 0.0287 - accuracy: 0.9906 - val_loss: 3.8755 - val_accuracy: 0.4450 - 1s/epoch - 89ms/step\n",
      "Epoch 98/100\n",
      "16/16 - 2s - loss: 0.0382 - accuracy: 0.9872 - val_loss: 3.9255 - val_accuracy: 0.4340 - 2s/epoch - 103ms/step\n",
      "Epoch 99/100\n",
      "16/16 - 2s - loss: 0.0493 - accuracy: 0.9845 - val_loss: 3.9712 - val_accuracy: 0.4275 - 2s/epoch - 108ms/step\n",
      "Epoch 100/100\n",
      "16/16 - 2s - loss: 0.0480 - accuracy: 0.9841 - val_loss: 4.0087 - val_accuracy: 0.4195 - 2s/epoch - 115ms/step\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 4.0068 - accuracy: 0.4173\n",
      "[4.006821155548096, 0.4172999858856201]\n"
     ]
    }
   ],
   "source": [
    "# 1사이클 클래스\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n",
    "        self.total_iteration = iterations  # 총 학습률 조정 반복 횟수\n",
    "        self.max_rate = max_rate  # 최대 학습률\n",
    "        self.start_rate = start_rate or max_rate / 10  # 시작 학습률 (디폴트는 최대 학습률의 10%)\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1  # 마지막 단계의 반복 횟수 (디폴트는 총 반복 횟수의 10%)\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2  # 중간 단계 반복 횟수\n",
    "        self.last_rate = last_rate or self.start_rate / 1000  # 마지막 학습률 (디폴트는 시작 학습률의 1/1000)\n",
    "        self.current_iteration = 0  # 현재 반복 횟수 초기화\n",
    "\n",
    "    def _interpolate(self, from_iter, to_iter2, from_rate, to_rate):\n",
    "        # 두 지점 사이에서 선형 보간을 통해 학습률 계산하여 to_iter까지 선형적으로 rate 변화\n",
    "        return ((to_rate - from_rate) * (self.current_iteration - from_iter) / (to_iter2 - from_iter) + from_rate)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.current_iteration < self.half_iteration:\n",
    "            # 초기 상승 단계\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.current_iteration < 2 * self.half_iteration:\n",
    "            # 최대 학습률로 상승한 후 하락 단계\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            # 마지막 하락 단계\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.total_iteration, self.start_rate, self.last_rate)\n",
    "        self.current_iteration += 1  # 반복 횟수 증가\n",
    "        self.model.optimizer.lr = rate  # 모델의 학습률 업데이트\n",
    "\n",
    "class MakeModel(keras.models.Sequential):\n",
    "    def __init__(self, input_num, hidden_nums, output_num, **kwargs):\n",
    "        super().__init__()\n",
    "        self.add(keras.layers.Input(shape=input_num))\n",
    "        self.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "        for hidden_num in hidden_nums:\n",
    "            self.add(keras.layers.Dense(hidden_num, 'selu', kernel_initializer='lecun_normal'))\n",
    "            self.add(keras.layers.BatchNormalization())\n",
    "        self.add(keras.layers.Dense(10, 'softmax'))\n",
    "        self.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "\n",
    "model = MakeModel((32, 32, 3), h_ls, 10)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=512, validation_data=(X_valid_scaled, y_valid), verbose=2, \n",
    "                    callbacks=[keras.callbacks.TensorBoard(get_run_logdir(5))])\n",
    "print(model.evaluate(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
