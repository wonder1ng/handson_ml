{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방지 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.putenv('TF_GPU_ALLOCATOR', 'cuda_malloc_async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[10 11 12 13]\n",
      " [ 8  9 10 11]\n",
      " [ 0  1  2  3]]\n",
      "===== \n",
      "Y_batch\n",
      "[[11 12 13 14]\n",
      " [ 9 10 11 12]\n",
      " [ 1  2  3  4]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[2 3 4 5]\n",
      " [6 7 8 9]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 3  4  5  6]\n",
      " [ 7  8  9 10]\n",
      " [ 5  6  7  8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in tf.data.Dataset.from_tensor_slices(tf.range(15)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6, 9, 8, 3]]\n",
      "['f i r s t']\n",
      "39 1115394\n",
      "(1115394,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "print(tokenizer.texts_to_sequences(['First']))\n",
    "print(tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(['First'])))\n",
    "max_id = len(tokenizer.word_index)  # 고유한 문자 갯수\n",
    "dataset_size = tokenizer.document_count # 전체 문자 갯수\n",
    "print(max_id, dataset_size)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1   # 1~39를 0~38로 변환하기 위해 -1\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 1115394\n"
     ]
    }
   ],
   "source": [
    "# train_size = dataset_size * 90 // 100\n",
    "train_size = dataset_size * 90 // 50000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "print(train_size, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target: 다음 1글자 앞의 input\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 41s 619ms/step - loss: 3.1098\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 39s 642ms/step - loss: 2.5213\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 46s 769ms/step - loss: 2.1505\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 44s 735ms/step - loss: 1.9232\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 41s 681ms/step - loss: 1.6984\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 47s 788ms/step - loss: 1.4702\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 43s 714ms/step - loss: 1.2467\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 46s 760ms/step - loss: 1.0494\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 47s 785ms/step - loss: 0.8860\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 56s 931ms/step - loss: 0.7547\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "                    #  dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "                    #  dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 10, 39), dtype=float32)\n",
      "\n",
      "[[ 1 13  1  2  9  1  0  2  3 13]]\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "y_pred = np.argmax(model(X_new), axis=-1)\n",
    "print(X_new)\n",
    "print()\n",
    "print(y_pred)\n",
    "print(tokenizer.sequences_to_texts(y_pred + 1)[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t the object of our misery, is as an\n",
      "inventory to p\n",
      "==============================\n",
      "w bat sole, to the commontly.\n",
      "\n",
      "second citizen:\n",
      "wear\n",
      "==============================\n",
      "w ze humsnll to lar ness't ai\n",
      ", but they, bf tye is\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=0.2))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=1))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 9s 324ms/step - loss: 3.4085\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 6s 321ms/step - loss: 3.0826\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 6s 320ms/step - loss: 3.0255\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 7s 345ms/step - loss: 2.9721\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 7s 331ms/step - loss: 2.8912\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 7s 337ms/step - loss: 2.7755\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 7s 337ms/step - loss: 2.6375\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 7s 343ms/step - loss: 2.5332\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 7s 376ms/step - loss: 2.4522\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 8s 408ms/step - loss: 2.4032\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 7s 359ms/step - loss: 2.3530\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 7s 374ms/step - loss: 2.2816\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 9s 466ms/step - loss: 2.2527\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 8s 390ms/step - loss: 2.2385\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 8s 384ms/step - loss: 2.1945\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 9s 440ms/step - loss: 2.2053\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 9s 444ms/step - loss: 2.1239\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 10s 518ms/step - loss: 2.1250\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 9s 426ms/step - loss: 2.0728\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 8s 426ms/step - loss: 2.0503\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 8s 408ms/step - loss: 2.0314\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 8s 404ms/step - loss: 2.0076\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 8s 389ms/step - loss: 1.9730\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 8s 399ms/step - loss: 1.9839\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 9s 442ms/step - loss: 1.9276\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 7s 333ms/step - loss: 1.9264\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 6s 320ms/step - loss: 1.8928\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 7s 338ms/step - loss: 1.8504\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 7s 327ms/step - loss: 1.8062\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 7s 348ms/step - loss: 1.8263\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 7s 334ms/step - loss: 1.7995\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 7s 355ms/step - loss: 1.7622\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 7s 355ms/step - loss: 1.7840\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 8s 400ms/step - loss: 1.7689\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 8s 394ms/step - loss: 1.6945\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 8s 406ms/step - loss: 1.6693\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 9s 450ms/step - loss: 1.6900\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 8s 416ms/step - loss: 1.6457\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 8s 378ms/step - loss: 1.6194\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 8s 398ms/step - loss: 1.6031\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 8s 397ms/step - loss: 1.5732\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 8s 378ms/step - loss: 1.5758\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 7s 362ms/step - loss: 1.5837\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 7s 358ms/step - loss: 1.5359\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 7s 349ms/step - loss: 1.5038\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 7s 345ms/step - loss: 1.4976\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 7s 326ms/step - loss: 1.4301\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 7s 326ms/step - loss: 1.4701\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 7s 332ms/step - loss: 1.4644\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 7s 341ms/step - loss: 1.3910\n"
     ]
    }
   ],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \n",
    "                     batch_input_shape=[1, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tizen:\n",
      "what he paties in what he cane to that he ha\n",
      "==============================\n",
      "wag is fur aciun:\n",
      "whll is revent: th prouslf\n",
      "is cou\n",
      "==============================\n",
      "wen:\n",
      "wh msmere\n",
      "?\n",
      "cul\n",
      "lusu\n",
      "ianthnn.\n",
      "woud,spit$ug,?\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=0.2))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=1))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os, numpy as np\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방지 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.putenv('TF_GPU_ALLOCATOR', 'cuda_malloc_async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n",
      "<sos> this film was just brilliant casting location scenery story\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "print(\" \".join(id_to_word[id_] for id_ in X_train[0][:10]))\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b'<br\\\\s*/?>', b' ')\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b' ')\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "\n",
    "    return X_batch.to_tensor(default_value=b'<pad>'), y_batch\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "print(vocabulary.most_common()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 41s 47ms/step - loss: 0.5397 - accuracy: 0.7184\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 43s 54ms/step - loss: 0.3398 - accuracy: 0.8593\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.1874 - accuracy: 0.9330\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.1420 - accuracy: 0.9484\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 41s 53ms/step - loss: 0.0996 - accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # 0 값을 패딩으로 간주하여 무시\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 47s 55ms/step - loss: 0.5439 - accuracy: 0.7202\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 43s 54ms/step - loss: 0.3751 - accuracy: 0.8380\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 44s 57ms/step - loss: 0.2256 - accuracy: 0.9162\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.1390 - accuracy: 0.9502\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.1331 - accuracy: 0.9481\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "# 0=False 나머지는 True로 하여 패딩값 무시하도록 함.\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5459 - accuracy: 0.7263\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5147 - accuracy: 0.7478\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5093 - accuracy: 0.7505\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5057 - accuracy: 0.7537\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5027 - accuracy: 0.7551\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim50/1',\n",
    "                   dtype=tf.string, input_shape=[], output_shape=50),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets['train'].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 19s 23ms/step - loss: 0.3756 - accuracy: 0.8406\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3251 - accuracy: 0.8593\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3201 - accuracy: 0.8627\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3157 - accuracy: 0.8656\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3113 - accuracy: 0.8675\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3066 - accuracy: 0.8695\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3019 - accuracy: 0.8722\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.2972 - accuracy: 0.8751\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2922 - accuracy: 0.8766\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2871 - accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # trainable=True로 할 경우 코랩에서 메모리 부족 에러가 발생합니다.\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                   trainable=False, dtype=tf.string, input_shape=[]),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 7s 158ms/step - loss: 6.1568\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 5s 160ms/step - loss: 4.6670\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=encoder_state, \n",
    "                                                             sequence_length=sequence_lengths)\n",
    "y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], y, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_width = 10\n",
    "# decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "#     cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
    "# decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(encoder_state, multiplier=beam_width)\n",
    "# outputs, _, _ = decoder(embedding_decoder, start_tokens=start_tokens, end_token=end_token, \n",
    "#                         initial_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os, numpy as np\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방지 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.putenv('TF_GPU_ALLOCATOR', 'cuda_malloc_async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have no further questions. => No tengo más preguntas.\n",
      "Tom is being hunted by the police. => Tom está siendo perseguido por la policía.\n",
      "I want to go to bed early. => Quiero irme temprano a la cama.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)  # 쌍을 2개의 리스트로 분리합니다.\n",
    "\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
    "\n",
    "print(text_vec_layer_en.get_vocabulary()[:10])\n",
    "print(text_vec_layer_es.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 116s 34ms/step - loss: 0.4154 - accuracy: 0.4248 - val_loss: 0.3110 - val_accuracy: 0.5223\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 133s 42ms/step - loss: 0.2654 - accuracy: 0.5718 - val_loss: 0.2394 - val_accuracy: 0.6044\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 144s 46ms/step - loss: 0.2087 - accuracy: 0.6430 - val_loss: 0.2090 - val_accuracy: 0.6448\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 147s 47ms/step - loss: 0.1760 - accuracy: 0.6873 - val_loss: 0.1947 - val_accuracy: 0.6659\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 147s 47ms/step - loss: 0.1528 - accuracy: 0.7203 - val_loss: 0.1878 - val_accuracy: 0.6746\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 130s 42ms/step - loss: 0.1342 - accuracy: 0.7479 - val_loss: 0.1859 - val_accuracy: 0.6809\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 97s 31ms/step - loss: 0.1181 - accuracy: 0.7731 - val_loss: 0.1879 - val_accuracy: 0.6800\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 94s 30ms/step - loss: 0.1038 - accuracy: 0.7961 - val_loss: 0.1913 - val_accuracy: 0.6802\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 95s 30ms/step - loss: 0.0913 - accuracy: 0.8174 - val_loss: 0.1976 - val_accuracy: 0.6782\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 94s 30ms/step - loss: 0.0806 - accuracy: 0.8360 - val_loss: 0.2027 - val_accuracy: 0.6765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29e9eeaaca0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "me gusta el fútbol\n",
      "==============================\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "me gusta [UNK] y a la playa no me gusta\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  # encoder input \n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n",
    "\n",
    "print(translate(\"I like soccer\"))\n",
    "print('='*30)\n",
    "print(translate(\"I like soccer and also going to the beach\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 122s 36ms/step - loss: 0.3101 - accuracy: 0.5366 - val_loss: 0.2219 - val_accuracy: 0.6288\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 109s 35ms/step - loss: 0.1923 - accuracy: 0.6682 - val_loss: 0.1884 - val_accuracy: 0.6751\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 109s 35ms/step - loss: 0.1604 - accuracy: 0.7119 - val_loss: 0.1766 - val_accuracy: 0.6910\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 109s 35ms/step - loss: 0.1393 - accuracy: 0.7414 - val_loss: 0.1732 - val_accuracy: 0.6952\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 113s 36ms/step - loss: 0.1226 - accuracy: 0.7664 - val_loss: 0.1717 - val_accuracy: 0.6998\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 118s 38ms/step - loss: 0.1082 - accuracy: 0.7894 - val_loss: 0.1754 - val_accuracy: 0.6992\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 111s 35ms/step - loss: 0.0960 - accuracy: 0.8092 - val_loss: 0.1783 - val_accuracy: 0.6982\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 107s 34ms/step - loss: 0.0850 - accuracy: 0.8283 - val_loss: 0.1843 - val_accuracy: 0.6958\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 107s 34ms/step - loss: 0.0758 - accuracy: 0.8442 - val_loss: 0.1899 - val_accuracy: 0.6932\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 0.0681 - accuracy: 0.8569 - val_loss: 0.1975 - val_accuracy: 0.6882\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "me gusta el fútbol\n",
      "==============================\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "me gusta la playa y a la [UNK] no le gusta\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(256, return_state=True))\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # 단기 상태 (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # 장기 상태 (1 & 3)\n",
    "# 추가 코드 - 모델을 완성하고 학습시킵니다.\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "output_layer = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))\n",
    "print(translate(\"I like soccer\"))\n",
    "print('='*30)\n",
    "print(translate(\"I like soccer and also going to the beach\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "me gustan los perros y los gatos\n",
      "==============================\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "상위 첫 단어: [(-0.03524482, 'me'), (-4.074361, 'yo'), (-4.891246, 'amo')]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "지금까지 최상의 번역: [(-0.17349973, 'me gustan'), (-2.3978343, 'me [UNK]'), (-3.8907247, 'me encanta')]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "지금까지 최상의 번역: [(-0.44391635, 'me gustan los'), (-1.7447308, 'me gustan tanto'), (-2.5370724, 'me [UNK] tanto')]\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "지금까지 최상의 번역: [(-1.0484011, 'me gustan los perros'), (-1.235138, 'me gustan los gatos'), (-1.857966, 'me gustan tanto los')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "지금까지 최상의 번역: [(-1.0535488, 'me gustan los perros y'), (-1.2409387, 'me gustan los gatos y'), (-1.8744655, 'me gustan tanto los perros')]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "지금까지 최상의 번역: [(-1.093529, 'me gustan los perros y los'), (-1.2701619, 'me gustan los gatos y los'), (-2.6374135, 'me gustan tanto los perros como')]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "지금까지 최상의 번역: [(-1.1862626, 'me gustan los perros y los gatos'), (-1.4132091, 'me gustan los gatos y los perros'), (-2.6423075, 'me gustan tanto los perros como los')]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "지금까지 최상의 번역: [(-1.1906064, 'me gustan los perros y los gatos endofseq'), (-1.4254812, 'me gustan los gatos y los perros endofseq'), (-2.8568635, 'me gustan tanto los perros como los gatos')]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "지금까지 최상의 번역: [(-1.1906064, 'me gustan los perros y los gatos endofseq'), (-1.4254812, 'me gustan los gatos y los perros endofseq'), (-2.8569088, 'me gustan tanto los perros como los gatos endofseq')]\n",
      "me gustan los perros y los gatos\n"
     ]
    }
   ],
   "source": [
    "# 추가 코드 - 빔 검색의 기본 구현\n",
    "\n",
    "def beam_search(sentence_en, beam_width, verbose=False):\n",
    "    X = np.array([sentence_en])  # 인코더 입력\n",
    "    X_dec = np.array([\"startofseq\"])  # 디코더 입력\n",
    "    y_proba = model.predict((X, X_dec))[0, 0]  # 첫 번째 토큰의 확률\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    top_translations = [  # 촤상의 (log_proba, translation) 리스트 \n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
    "    ]\n",
    "    \n",
    "    # 추가 코드 - verbose 모드에서 상위 첫 단어를 표시합니다.\n",
    "    if verbose:\n",
    "        print(\"상위 첫 단어:\", top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(\"endofseq\"):\n",
    "                candidates.append((log_proba, translation))\n",
    "                continue  # 번역이 완료되었으므로 번역을 이어가지 않습니다.\n",
    "            X = np.array([sentence_en])  # 인코더 입력\n",
    "            X_dec = np.array([\"startofseq \" + translation])  # 디코더 입력\n",
    "            y_proba = model.predict((X, X_dec))[0, idx]  # 마지막 토큰의 확률\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba),\n",
    "                                   f\"{translation} {word}\"))\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # 추가 코드 - verbose 모드의 경우 지금까지의 최상의 번역을 출력합니다.\n",
    "        if verbose:\n",
    "            print(\"지금까지 최상의 번역:\", top_translations)\n",
    "\n",
    "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
    "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()\n",
    "\n",
    "# 추가 코드 - 모델이 어떻게 오류를 발생시키는지 보여줍니다.\n",
    "sentence_en = \"I love cats and dogs\"\n",
    "print(translate(sentence_en))\n",
    "print('='*30)\n",
    "print(beam_search(sentence_en, beam_width=3, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 134s 40ms/step - loss: 0.3024 - accuracy: 0.5544 - val_loss: 0.2100 - val_accuracy: 0.6566\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 125s 40ms/step - loss: 0.1888 - accuracy: 0.6819 - val_loss: 0.1866 - val_accuracy: 0.6881\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 116s 37ms/step - loss: 0.1656 - accuracy: 0.7130 - val_loss: 0.1774 - val_accuracy: 0.6995\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 121s 39ms/step - loss: 0.1497 - accuracy: 0.7345 - val_loss: 0.1738 - val_accuracy: 0.7053\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 122s 39ms/step - loss: 0.1370 - accuracy: 0.7530 - val_loss: 0.1739 - val_accuracy: 0.7079\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 118s 38ms/step - loss: 0.1260 - accuracy: 0.7689 - val_loss: 0.1747 - val_accuracy: 0.7113\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 120s 38ms/step - loss: 0.1167 - accuracy: 0.7828 - val_loss: 0.1782 - val_accuracy: 0.7099\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 109s 35ms/step - loss: 0.1085 - accuracy: 0.7953 - val_loss: 0.1811 - val_accuracy: 0.7101\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 110s 35ms/step - loss: 0.1015 - accuracy: 0.8063 - val_loss: 0.1856 - val_accuracy: 0.7083\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 128s 41ms/step - loss: 0.0953 - accuracy: 0.8164 - val_loss: 0.1902 - val_accuracy: 0.7071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29fbfe54a30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True, return_state=True))\n",
    "\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "attention_layer = keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "y_proba = output_layer(attention_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "me gusta el fútbol y también ir a la playa\n",
      "==============================\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "상위 첫 단어: [(-0.074255265, 'me'), (-4.6216154, 'yo'), (-4.801601, 'el')]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "지금까지 최상의 번역: [(-0.090356335, 'me gusta'), (-4.5053167, 'me encanta'), (-4.6623745, 'yo gusta')]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "지금까지 최상의 번역: [(-0.2396906, 'me gusta el'), (-3.2079437, 'me gusta fútbol'), (-3.333143, 'me gusta [UNK]')]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "지금까지 최상의 번역: [(-0.24173188, 'me gusta el fútbol'), (-3.2113903, 'me gusta fútbol y'), (-3.8187408, 'me gusta [UNK] y')]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "지금까지 최상의 번역: [(-0.24807271, 'me gusta el fútbol y'), (-3.2279015, 'me gusta fútbol y también'), (-3.9083428, 'me gusta [UNK] y también')]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "지금까지 최상의 번역: [(-0.29714185, 'me gusta el fútbol y también'), (-4.548584, 'me gusta fútbol y también [UNK]'), (-4.695448, 'me gusta el fútbol y [UNK]')]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "지금까지 최상의 번역: [(-1.7989283, 'me gusta el fútbol y también ir'), (-1.8607802, 'me gusta el fútbol y también [UNK]'), (-2.0088303, 'me gusta el fútbol y también va')]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "지금까지 최상의 번역: [(-1.9082049, 'me gusta el fútbol y también ir a'), (-2.0929394, 'me gusta el fútbol y también va a'), (-2.7692094, 'me gusta el fútbol y también [UNK] la')]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "지금까지 최상의 번역: [(-1.9270161, 'me gusta el fútbol y también ir a la'), (-2.366804, 'me gusta el fútbol y también va a la'), (-2.769612, 'me gusta el fútbol y también [UNK] la playa')]\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "지금까지 최상의 번역: [(-1.9273741, 'me gusta el fútbol y también ir a la playa'), (-2.3671646, 'me gusta el fútbol y también va a la playa'), (-2.7767534, 'me gusta el fútbol y también [UNK] la playa endofseq')]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "지금까지 최상의 번역: [(-1.9529611, 'me gusta el fútbol y también ir a la playa endofseq'), (-2.3921025, 'me gusta el fútbol y también va a la playa endofseq'), (-2.7767534, 'me gusta el fútbol y también [UNK] la playa endofseq')]\n",
      "me gusta el fútbol y también ir a la playa\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"I like soccer and also going to the beach\"))\n",
    "print('='*30)\n",
    "print(beam_search(\"I like soccer and also going to the beach\", beam_width=3, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims%2 == 1:\n",
    "            max_dims += 1   # max_dims는 짝수여야 함\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims//2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        \n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "max_steps = 500\n",
    "vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)\n",
    "\n",
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    X = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    # Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z], use_causal_mask=True)    # tensorflow WARNING에 따라 변경\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation='softmax'))(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50  # 전체 훈련 세트에 있는 최대 길이\n",
    "embed_size = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "pos_embed_layer = keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\n",
    "\n",
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    \n",
    "    skip = Z\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 406s 128ms/step - loss: 1.0423 - accuracy: 0.8620 - val_loss: 0.6979 - val_accuracy: 0.8774\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 409s 131ms/step - loss: 0.7588 - accuracy: 0.8832 - val_loss: 0.5549 - val_accuracy: 0.9011\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 404s 129ms/step - loss: 0.7956 - accuracy: 0.8807 - val_loss: 1.0742 - val_accuracy: 0.8583\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 394s 126ms/step - loss: 0.7040 - accuracy: 0.8840 - val_loss: 0.6085 - val_accuracy: 0.8946\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 397s 127ms/step - loss: 0.5611 - accuracy: 0.8989 - val_loss: 0.5584 - val_accuracy: 0.9006\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 395s 127ms/step - loss: 0.5029 - accuracy: 0.9050 - val_loss: 0.4864 - val_accuracy: 0.9066\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 396s 127ms/step - loss: 0.4726 - accuracy: 0.9075 - val_loss: 0.4592 - val_accuracy: 0.9087\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 400s 128ms/step - loss: 0.4537 - accuracy: 0.9092 - val_loss: 0.4409 - val_accuracy: 0.9104\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 398s 127ms/step - loss: 0.4391 - accuracy: 0.9108 - val_loss: 0.4340 - val_accuracy: 0.9116\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 396s 127ms/step - loss: 0.4280 - accuracy: 0.9122 - val_loss: 0.4183 - val_accuracy: 0.9138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29f06fc8790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_max_len_dec = 50  # tf.shape(decoder_embeddings)[1]\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, None]\n",
    "causal_mask = tf.linalg.band_part(  # 행렬의 대각선 부분을 추출하거나 수정하는 함수\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask&decoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "\n",
    "y_proba = keras.layers.Dense(vocab_size, activation='softmax')(Z)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")  # 다른 많은 작업을 사용할 수 있습니다.\n",
    "result = classifier(\"The actors were very convincing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
       " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I am from India.\", \"I am from Iraq.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'contradiction', 'score': 0.9790191650390625}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([(\"I like soccer.\", \"We all love soccer!\"),\n",
    "                       (\"Joe lived for a very long time.\", \"Joe is old.\")],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-2.110418  ,  1.1792463 ,  1.4074315 ],\n",
       "       [-0.01550994,  1.0979388 , -0.99263567]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.01624809, 0.4359988 , 0.54775316],\n",
       "       [0.22618128, 0.6886861 , 0.08513268]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = keras.activations.softmax(outputs.logits)\n",
    "Y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1], dtype=int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': 'January', '02': 'February', '03': 'March', '04': 'April', '05': 'May', '06': 'June', '07': 'July', '08': 'August', '09': 'September', '10': 'October', '11': 'November', '12': 'December'}\n",
      "November 11, 4281 3287182 4281-11-11 3287182\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, keras, tensorflow as tf\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "MONTHS_DICT = {f'{i:>02}': v for i, v in enumerate(MONTHS, 1)}\n",
    "print(MONTHS_DICT)\n",
    "\n",
    "start_date = np.datetime64('1000-01-01')\n",
    "end_date = np.datetime64('9999-12-31')\n",
    "y = np.arange(start_date, end_date + np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "np.random.shuffle(y)\n",
    "y = list(map(str, y))\n",
    "X = list(map(lambda x: MONTHS_DICT[x[5:7]] + ' ' + x[-2:] + ', ' + x[:4], y))\n",
    "print(X[0], len(X), y[0], len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,0123456789ADFJMNOSabceghilmnoprstuvy\n"
     ]
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "# MONTHS를 join 후 set으로 중복 문자 제거한 후 숫자와 쉼표, 공백을 포함하여 입력값 원소를 정렬하여 문자열로 생성\n",
    "print(INPUT_CHARS)\n",
    "OUTPUT_CHARS = \"0123456789-\"    # 출력값 원소를 문자열로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str_to_ids = lambda x:list(map(OUTPUT_CHARS.index, x))\n",
    "def prepare_date_strs(date_strs, date_str_to_ids=date_str_to_ids):\n",
    "    X_ids = list(map(date_str_to_ids, date_strs))\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
    "\n",
    "Y_train = prepare_date_strs(y[:10000])\n",
    "Y_valid = prepare_date_strs(y[10000:12000])\n",
    "Y_test = prepare_date_strs(y[12000:14000])\n",
    "\n",
    "date_str_to_ids = lambda x:list(map(INPUT_CHARS.index, x))\n",
    "def prepare_date_strs(date_strs, date_str_to_ids=date_str_to_ids):\n",
    "    X_ids = list(map(date_str_to_ids, date_strs))\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
    "\n",
    "X_train = prepare_date_strs(X[:10000])\n",
    "X_valid = prepare_date_strs(X[10000:12000])\n",
    "X_test = prepare_date_strs(X[12000:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 32)          1248      \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               82432     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 10, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 10, 128)           131584    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10, 12)            1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,812\n",
      "Trainable params: 216,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = 10\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS)+1, output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS)+1, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 6s 15ms/step - loss: 1.8064 - accuracy: 0.3476 - val_loss: 1.4035 - val_accuracy: 0.4661\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.3494 - accuracy: 0.5127 - val_loss: 1.1450 - val_accuracy: 0.5968\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.1790 - accuracy: 0.5905 - val_loss: 0.9765 - val_accuracy: 0.6460\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.8473 - accuracy: 0.6849 - val_loss: 0.7362 - val_accuracy: 0.7140\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6501 - accuracy: 0.7463 - val_loss: 0.5763 - val_accuracy: 0.7707\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6280 - accuracy: 0.7674 - val_loss: 0.4504 - val_accuracy: 0.8293\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4239 - accuracy: 0.8437 - val_loss: 0.3076 - val_accuracy: 0.8802\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.2459 - accuracy: 0.9138 - val_loss: 0.1979 - val_accuracy: 0.9349\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.2806 - accuracy: 0.9204 - val_loss: 0.1550 - val_accuracy: 0.9581\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1616 - accuracy: 0.9644 - val_loss: 0.4901 - val_accuracy: 0.8356\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1027 - accuracy: 0.9778 - val_loss: 0.0587 - val_accuracy: 0.9901\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0414 - accuracy: 0.9947 - val_loss: 0.0370 - val_accuracy: 0.9950\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.0259 - accuracy: 0.9978 - val_loss: 0.0238 - val_accuracy: 0.9977\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 0.0166 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0118 - accuracy: 0.9996 - val_loss: 0.0120 - val_accuracy: 0.9993\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0084 - accuracy: 0.9998 - val_loss: 0.0086 - val_accuracy: 0.9996\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0061 - accuracy: 0.9999 - val_loss: 0.0066 - val_accuracy: 0.9995\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 0.9999\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9999\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0029835947789251804, 1.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('nadam', 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, 32, 20, validation_data=(X_valid, Y_valid))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 430ms/step\n",
      "2020-02-02\n",
      "1789-02-14\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "2020-05-02\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]\n",
    "\n",
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])\n",
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)\n",
    "\n",
    "\n",
    "def prepare_date_strs_padded(date_strs, max_length=X_train.shape[1]):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_length - X.shape[1]]])\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_new = prepare_date_strs_padded([\"May 02, 2020\", \"July 14, 1789\"])\n",
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': 'January', '02': 'February', '03': 'March', '04': 'April', '05': 'May', '06': 'June', '07': 'July', '08': 'August', '09': 'September', '10': 'October', '11': 'November', '12': 'December'}\n",
      "August 25, 1918 3287182 1918-08-25 3287182\n",
      " ,0123456789ADFJMNOSabceghilmnoprstuvy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, keras, tensorflow as tf\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "MONTHS_DICT = {f'{i:>02}': v for i, v in enumerate(MONTHS, 1)}\n",
    "print(MONTHS_DICT)\n",
    "\n",
    "start_date = np.datetime64('1000-01-01')\n",
    "end_date = np.datetime64('9999-12-31')\n",
    "y = np.arange(start_date, end_date + np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "np.random.shuffle(y)\n",
    "y = list(map(str, y))\n",
    "X = list(map(lambda x: MONTHS_DICT[x[5:7]] + ' ' + x[-2:] + ', ' + x[:4], y))\n",
    "\n",
    "\n",
    "print(X[0], len(X), y[0], len(y))\n",
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "# MONTHS를 join 후 set으로 중복 문자 제거한 후 숫자와 쉼표, 공백을 포함하여 입력값 원소를 정렬하여 문자열로 생성\n",
    "print(INPUT_CHARS)\n",
    "OUTPUT_CHARS = \"0123456789-\"    # 출력값 원소를 문자열로 생성\n",
    "\n",
    "\n",
    "date_str_to_ids = lambda x:list(map(OUTPUT_CHARS.index, x))\n",
    "def prepare_date_strs(date_strs, date_str_to_ids=date_str_to_ids):\n",
    "    X_ids = list(map(date_str_to_ids, date_strs))\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
    "\n",
    "Y_train = prepare_date_strs(y[:10000])\n",
    "Y_valid = prepare_date_strs(y[10000:12000])\n",
    "Y_test = prepare_date_strs(y[12000:14000])\n",
    "\n",
    "date_str_to_ids = lambda x:list(map(INPUT_CHARS.index, x))\n",
    "def prepare_date_strs(date_strs, date_str_to_ids=date_str_to_ids):\n",
    "    X_ids = list(map(date_str_to_ids, date_strs))\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
    "\n",
    "X_train = prepare_date_strs(X[:10000])\n",
    "X_valid = prepare_date_strs(X[10000:12000])\n",
    "X_test = prepare_date_strs(X[12000:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 32)     1248        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, None, 32)     416         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)                  [(None, 128),        82432       ['embedding_7[0][0]']            \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)                  (None, None, 128)    82432       ['embedding_8[0][0]',            \n",
      "                                                                  'lstm_8[0][1]',                 \n",
      "                                                                  'lstm_8[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, None, 12)     1548        ['lstm_9[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 168,076\n",
      "Trainable params: 168,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2, output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 7s 16ms/step - loss: 1.6888 - accuracy: 0.3598 - val_loss: 1.4800 - val_accuracy: 0.4083\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.3470 - accuracy: 0.4816 - val_loss: 1.1992 - val_accuracy: 0.5470\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.9443 - accuracy: 0.6512 - val_loss: 0.6522 - val_accuracy: 0.7677\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.4352 - accuracy: 0.8586 - val_loss: 0.2382 - val_accuracy: 0.9395\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.1397 - accuracy: 0.9762 - val_loss: 0.0755 - val_accuracy: 0.9932\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0496 - accuracy: 0.9976 - val_loss: 0.0383 - val_accuracy: 0.9977\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.0734 - accuracy: 0.9888 - val_loss: 0.0256 - val_accuracy: 0.9994\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0171 - accuracy: 0.9999 - val_loss: 0.0140 - val_accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "tf.Tensor(\n",
      "[[12  2  8  9 10 11  1  8 11  2  5]\n",
      " [12  3  1  3  1 11  1  6 11  1  2]], shape=(2, 11), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    print(Y_pred)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])\n",
    "\n",
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
