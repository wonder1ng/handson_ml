{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방지 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.putenv('TF_GPU_ALLOCATOR', 'cuda_malloc_async')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[ 4  5  6  7]\n",
      " [10 11 12 13]\n",
      " [ 6  7  8  9]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 5  6  7  8]\n",
      " [11 12 13 14]\n",
      " [ 7  8  9 10]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 8  9 10 11]\n",
      " [ 2  3  4  5]\n",
      " [ 0  1  2  3]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 9 10 11 12]\n",
      " [ 3  4  5  6]\n",
      " [ 1  2  3  4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in tf.data.Dataset.from_tensor_slices(tf.range(15)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 6, 9, 8, 3]]\n",
      "['f i r s t']\n",
      "39 1115394\n",
      "(1115394,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "print(tokenizer.texts_to_sequences(['First']))\n",
    "print(tokenizer.sequences_to_texts(tokenizer.texts_to_sequences(['First'])))\n",
    "max_id = len(tokenizer.word_index)  # 고유한 문자 갯수\n",
    "dataset_size = tokenizer.document_count # 전체 문자 갯수\n",
    "print(max_id, dataset_size)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1   # 1~39를 0~38로 변환하기 위해 -1\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 1115394\n"
     ]
    }
   ],
   "source": [
    "# train_size = dataset_size * 90 // 100\n",
    "train_size = dataset_size * 90 // 50000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "print(train_size, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 51s 808ms/step - loss: 3.1075\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 53s 872ms/step - loss: 2.5708\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 72s 1s/step - loss: 2.1977\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 89s 1s/step - loss: 1.9609\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 69s 1s/step - loss: 1.7232\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 77s 1s/step - loss: 1.4866\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 102s 2s/step - loss: 1.2604\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 76s 1s/step - loss: 1.0565\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 93s 2s/step - loss: 0.8922\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 102s 2s/step - loss: 0.7565\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "                    #  dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "                    #  dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "#                      #dropout=0.2, recurrent_dropout=0.2),\n",
    "#                      dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True,\n",
    "#                      #dropout=0.2, recurrent_dropout=0.2),\n",
    "#                      dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "#                                                     activation=\"softmax\"))\n",
    "# ])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# history = model.fit(dataset, batch_size=1024, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 10, 39), dtype=float32)\n",
      "\n",
      "[[ 1 13  1  2  9  0  0  4  3 13]]\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "y_pred = np.argmax(model(X_new), axis=-1)\n",
    "print(X_new)\n",
    "print()\n",
    "print(y_pred)\n",
    "print(tokenizer.sequences_to_texts(y_pred + 1)[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the object of our misery, is a gain to the commonal\n",
      "==============================\n",
      "w't, we know't.\n",
      "\n",
      "first citizen:\n",
      "he hath famousl, is\n",
      "==============================\n",
      "wzif3ricuull. be doney proud; sofkmry, goodeco3tt!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=0.2))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=1))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "# datasets = []\n",
    "# for encoded_part in encoded_parts:\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "#     dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "#     dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "#     datasets.append(dataset)\n",
    "# dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "# dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "# dataset = dataset.map(\n",
    "#     lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "# dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 9s 373ms/step - loss: 3.4010\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 8s 415ms/step - loss: 3.0855\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 9s 433ms/step - loss: 3.0326\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 20s 1s/step - loss: 2.9773\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 25s 1s/step - loss: 2.8936\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 27s 1s/step - loss: 2.7868\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 38s 2s/step - loss: 2.6691\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 18s 880ms/step - loss: 2.5556\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 14s 689ms/step - loss: 2.4478\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 10s 477ms/step - loss: 2.4099\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 10s 499ms/step - loss: 2.3616\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 10s 490ms/step - loss: 2.3143\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 11s 566ms/step - loss: 2.2713\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 12s 617ms/step - loss: 2.2320\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 13s 669ms/step - loss: 2.1998\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 15s 723ms/step - loss: 2.1953\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 18s 916ms/step - loss: 2.1510\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 16s 782ms/step - loss: 2.1347\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 12s 605ms/step - loss: 2.1099\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 10s 512ms/step - loss: 2.1107\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 11s 556ms/step - loss: 2.0445\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 13s 645ms/step - loss: 2.0277\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 16s 800ms/step - loss: 2.0225\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 19s 969ms/step - loss: 1.9673\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1.9586\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 15s 730ms/step - loss: 1.9469\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 12s 612ms/step - loss: 1.9184\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 11s 540ms/step - loss: 1.9039\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 12s 610ms/step - loss: 1.8710\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 11s 527ms/step - loss: 1.8690\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 12s 622ms/step - loss: 1.8553\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 14s 683ms/step - loss: 1.8141\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 17s 870ms/step - loss: 1.7856\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 21s 1s/step - loss: 1.7763\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 29s 1s/step - loss: 1.7578\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 9s 450ms/step - loss: 1.7162\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 9s 475ms/step - loss: 1.6759\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 9s 468ms/step - loss: 1.6736\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 10s 507ms/step - loss: 1.6306\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 10s 485ms/step - loss: 1.6403\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 9s 448ms/step - loss: 1.5938\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 10s 486ms/step - loss: 1.6005\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 9s 470ms/step - loss: 1.5531\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 21s 1s/step - loss: 1.5697\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 12s 617ms/step - loss: 1.5161\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 16s 826ms/step - loss: 1.4737\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 24s 1s/step - loss: 1.4779\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 13s 672ms/step - loss: 1.4278\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 13s 673ms/step - loss: 1.4558\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 9s 430ms/step - loss: 1.4115\n"
     ]
    }
   ],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, \n",
    "                     batch_input_shape=[1, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tizen: what he hamane the cannond to the cations.\n",
      "\n",
      "\n",
      "==============================\n",
      "wo reseak, net ento to patrly.\n",
      "\n",
      "second citizen:\n",
      "ins\n",
      "==============================\n",
      "wwt thay i\n",
      "fkak seir3 tsecwlvldg\n",
      "ne: pao!\n",
      "cudp:ygak\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', temperature=0.2))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=1))\n",
    "print('='*30)\n",
    "print(complete_text('w', temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n",
      "<sos> this film was just brilliant casting location scenery story\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "print(\" \".join(id_to_word[id_] for id_ in X_train[0][:10]))\n",
    "\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b'<br\\\\s*/?>', b' ')\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b' ')\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "\n",
    "    return X_batch.to_tensor(default_value=b'<pad>'), y_batch\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "print(vocabulary.most_common()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 46s 54ms/step - loss: 0.5383 - accuracy: 0.7187\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 41s 53ms/step - loss: 0.3407 - accuracy: 0.8566\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.1800 - accuracy: 0.9363\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.1372 - accuracy: 0.9507\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.1162 - accuracy: 0.9579\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 46s 53ms/step - loss: 0.5296 - accuracy: 0.7269\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.3477 - accuracy: 0.8549\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.1994 - accuracy: 0.9300\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.1369 - accuracy: 0.9517\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.1129 - accuracy: 0.9599\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5457 - accuracy: 0.7280\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5140 - accuracy: 0.7490\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5090 - accuracy: 0.7526\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5056 - accuracy: 0.7543\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5027 - accuracy: 0.7552\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim50/2',\n",
    "                   dtype=tf.string, input_shape=[], output_shape=50),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets['train'].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 19s 22ms/step - loss: 0.3764 - accuracy: 0.8409\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.3253 - accuracy: 0.8597\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3204 - accuracy: 0.8616\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3163 - accuracy: 0.8644\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3124 - accuracy: 0.8665\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3084 - accuracy: 0.8684\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3045 - accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3004 - accuracy: 0.8728\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2962 - accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2920 - accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x292d5e30040>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # trainable=True로 할 경우 코랩에서 메모리 부족 에러가 발생합니다.\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                   trainable=False, dtype=tf.string, input_shape=[]),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=encoder_state, \n",
    "                                                             sequence_length=sequence_lengths)\n",
    "y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], y, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_width = 10\n",
    "# decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "#     cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
    "# decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(encoder_state, multiplier=beam_width)\n",
    "# outputs, _, _ = decoder(embedding_decoder, start_tokens=start_tokens, end_token=end_token, \n",
    "#                         initial_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it was a misunderstanding. => Creo que fue un malentendido.\n",
      "He makes it a rule never to speak ill of others. => Él tiene por regla nunca hablar mal de otros.\n",
      "The new tunnel is twice as long as the old one. => El nuevo túnel es el doble de largo que el viejo.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)  # 쌍을 2개의 리스트로 분리합니다.\n",
    "\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
    "\n",
    "print(text_vec_layer_en.get_vocabulary()[:10])\n",
    "print(text_vec_layer_es.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "cosas llegar ese sólo cayó así padres hacer seguir hacer bastante bastante suficiente banco bastante ayer música música pienso pena pena perro conmigo estar más somos comprado café cabeza por por profesor profesor di completamente abrió mira profesor mucho seis único acaba minutos entiendo vista minutos vista minutos boca invierno\n",
      "==============================\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "alta rompió pensar rompió saber tren beber pero de quien largo guerra venga hacia siendo ayudar ayudar mes odio odio\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  # encoder input \n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n",
    "\n",
    "print(translate(\"I like soccer\"))\n",
    "print('='*30)\n",
    "print(translate(\"I like soccer and also going to the beach\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(256, return_state=True))\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # 단기 상태 (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # 장기 상태 (1 & 3)\n",
    "# 추가 코드 - 모델을 완성하고 학습시킵니다.\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "output_layer = keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))\n",
    "print(translate(\"I like soccer\"))\n",
    "print('='*30)\n",
    "print(translate(\"I like soccer and also going to the beach\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "comer corriendo abrir amor voz llevó listo ayuda inglés tienes colegio encontró ir ir cuesta quien algún las mismo contigo mismo podemos tres tres sos futuro futuro grande muchas pena haber manos precio iré iré policía mía comer ningún oportunidad montaña tenés cuántas zapatos zapatos zapatos comprado amiga amiga zapatos\n",
      "==============================\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "상위 첫 단어: [(-6.8989854, 'comer'), (-6.899154, 'zapatos'), (-6.8993587, 'les')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-13.793785, 'zapatos corriendo'), (-13.795052, 'zapatos comer'), (-13.796391, 'zapatos zapatos')]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "지금까지 최상의 번역: [(-20.687904, 'zapatos zapatos corriendo'), (-20.690207, 'zapatos zapatos comer'), (-20.690353, 'zapatos comer corriendo')]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "지금까지 최상의 번역: [(-27.582058, 'zapatos zapatos corriendo pensaba'), (-27.583464, 'zapatos zapatos corriendo comer'), (-27.583654, 'zapatos zapatos comer corriendo')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-34.47758, 'zapatos zapatos corriendo pensaba pensaba'), (-34.47813, 'zapatos zapatos comer corriendo pensaba'), (-34.479065, 'zapatos zapatos corriendo comer pensaba')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "지금까지 최상의 번역: [(-41.37188, 'zapatos zapatos corriendo pensaba pensaba cama'), (-41.37211, 'zapatos zapatos corriendo pensaba pensaba llegar'), (-41.374092, 'zapatos zapatos comer corriendo pensaba pensaba')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-48.26664, 'zapatos zapatos corriendo pensaba pensaba cama llegar'), (-48.26671, 'zapatos zapatos corriendo pensaba pensaba llegar rato'), (-48.267746, 'zapatos zapatos corriendo pensaba pensaba cama montón')]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-55.16204, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato'), (-55.163025, 'zapatos zapatos corriendo pensaba pensaba cama llegar noticia'), (-55.16342, 'zapatos zapatos corriendo pensaba pensaba cama llegar rato')]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-62.055817, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato era'), (-62.056602, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato luz'), (-62.05806, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "지금까지 최상의 번역: [(-68.94916, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato era'), (-68.94963, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz'), (-68.94984, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato luz mundo')]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "지금까지 최상의 번역: [(-75.84003, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz mundo'), (-75.84376, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz'), (-75.844246, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato era mundo')]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "지금까지 최상의 번역: [(-82.73257, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo'), (-82.73403, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz mundo mundo'), (-82.73644, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz mundo número')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-89.62578, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo'), (-89.62709, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz mundo mundo divertido'), (-89.627495, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz mundo mundo personas')]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "지금까지 최상의 번역: [(-96.51813, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido'), (-96.51932, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo personas'), (-96.520226, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo lo')]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "지금까지 최상의 번역: [(-103.4131, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido'), (-103.41331, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo personas fotos'), (-103.41335, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido menos')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "지금까지 최상의 번역: [(-110.30632, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido menos'), (-110.30736, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido ir'), (-110.307465, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo')]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "지금까지 최상의 번역: [(-117.20136, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido ir menos'), (-117.20162, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo'), (-117.20167, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo menos')]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-124.09623, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido'), (-124.09663, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo menos australia'), (-124.09738, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo menos pasar')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-130.99007, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido almuerzo'), (-130.99126, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo'), (-130.99161, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido podrías')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "지금까지 최상의 번역: [(-137.88422, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo'), (-137.88568, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido podrías almuerzo'), (-137.88625, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido podrías ni')]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "지금까지 최상의 번역: [(-144.77986, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo'), (-144.78015, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo casado'), (-144.78096, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido podrías ni almuerzo')]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-151.67426, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo casado'), (-151.67438, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta'), (-151.67668, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "지금까지 최상의 번역: [(-158.57028, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta tomar'), (-158.5706, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta ni'), (-158.57108, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado')]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "지금까지 최상의 번역: [(-165.46626, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta tomar futuro'), (-165.46733, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido'), (-165.4677, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta ni futuro')]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "지금까지 최상의 번역: [(-172.36317, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido'), (-172.3634, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido hospital'), (-172.36351, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo hasta tomar futuro futuro')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-179.2568, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital'), (-179.2581, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido dime'), (-179.25826, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido dijiste')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "지금까지 최상의 번역: [(-186.15375, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste'), (-186.15411, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital siento'), (-186.15411, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido dijiste nuestra')]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "지금까지 최상의 번역: [(-193.04962, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste oír'), (-193.04965, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir'), (-193.0502, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido dijiste nuestra pasando')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-199.94484, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír'), (-199.94601, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir mujer'), (-199.94658, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido dijiste nuestra pasando menos')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-206.84305, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía'), (-206.84349, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír ves'), (-206.84367, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír nosotros')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "지금까지 최상의 번역: [(-213.73909, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía nosotros'), (-213.74109, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros'), (-213.74115, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía primero')]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-220.63573, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño'), (-220.63715, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía nosotros londres'), (-220.6375, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía nosotros mañana')]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "지금까지 최상의 번역: [(-227.5329, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño'), (-227.53316, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño montón'), (-227.53358, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía nosotros mañana baño')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-234.42815, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó'), (-234.42882, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño comprado'), (-234.42926, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "지금까지 최상의 번역: [(-241.32419, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló'), (-241.32513, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre esperando'), (-241.32524, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó noticia')]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-248.22113, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre esperando trata'), (-248.22203, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó'), (-248.2223, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló sale')]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-255.11874, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre esperando trata ni'), (-255.11885, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló'), (-255.119, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre esperando trata quedé')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "지금까지 최상의 번역: [(-262.01422, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van'), (-262.0146, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño entre esperando trata ni pieza'), (-262.01538, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló puesto')]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "지금까지 최상의 번역: [(-268.90952, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van'), (-268.9099, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van escritorio'), (-268.91187, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van amo')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "지금까지 최상의 번역: [(-275.80334, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van escritorio'), (-275.80496, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van'), (-275.80606, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van llamó')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-282.69754, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio'), (-282.6989, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van escritorio escritorio'), (-282.69965, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van minutos')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-289.5923, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio'), (-289.5932, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio di'), (-289.59415, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio clase')]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "지금까지 최상의 번역: [(-296.4876, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio clase'), (-296.48785, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio di'), (-296.4881, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto')]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "지금까지 최상의 번역: [(-303.38187, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos'), (-303.38202, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto alguien'), (-303.38327, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio')]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "지금까지 최상의 번역: [(-310.2748, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio alguien'), (-310.27597, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos'), (-310.27646, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos bastante')]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "지금까지 최상의 번역: [(-317.16895, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante'), (-317.17004, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio alguien feliz'), (-317.17038, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio alguien par')]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-324.0634, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante'), (-324.0639, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio alguien par unos'), (-324.06464, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante habla')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "지금까지 최상의 번역: [(-330.95697, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante habla'), (-330.95837, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto escritorio alguien par unos unos'), (-330.95865, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-337.8515, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias habla'), (-337.8517, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante habla habla'), (-337.85184, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias noticias')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "지금까지 최상의 번역: [(-344.7433, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias noticias noticias'), (-344.74445, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias noticias tomó'), (-344.7447, 'zapatos zapatos corriendo pensaba pensaba llegar rato rato rato luz luz mundo mundo divertido divertido nuevo comiendo divertido nuevo almuerzo almuerzo pasar casado perdido perdido hospital dijiste ir oír todavía perros baño baño llamó habló llamó habló van van van escritorio escritorio asunto unos unos bastante bastante noticias noticias habla')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 추가 코드 - 빔 검색의 기본 구현\n",
    "\n",
    "def beam_search(sentence_en, beam_width, verbose=False):\n",
    "    X = np.array([sentence_en])  # 인코더 입력\n",
    "    X_dec = np.array([\"startofseq\"])  # 디코더 입력\n",
    "    y_proba = model.predict((X, X_dec))[0, 0]  # 첫 번째 토큰의 확률\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    top_translations = [  # 촤상의 (log_proba, translation) 리스트 \n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
    "    ]\n",
    "    \n",
    "    # 추가 코드 - verbose 모드에서 상위 첫 단어를 표시합니다.\n",
    "    if verbose:\n",
    "        print(\"상위 첫 단어:\", top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(\"endofseq\"):\n",
    "                candidates.append((log_proba, translation))\n",
    "                continue  # 번역이 완료되었으므로 번역을 이어가지 않습니다.\n",
    "            X = np.array([sentence_en])  # 인코더 입력\n",
    "            X_dec = np.array([\"startofseq \" + translation])  # 디코더 입력\n",
    "            y_proba = model.predict((X, X_dec))[0, idx]  # 마지막 토큰의 확률\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba),\n",
    "                                   f\"{translation} {word}\"))\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # 추가 코드 - verbose 모드의 경우 지금까지의 최상의 번역을 출력합니다.\n",
    "        if verbose:\n",
    "            print(\"지금까지 최상의 번역:\", top_translations)\n",
    "\n",
    "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
    "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()\n",
    "\n",
    "# 추가 코드 - 모델이 어떻게 오류를 발생시키는지 보여줍니다.\n",
    "sentence_en = \"I love cats and dogs\"\n",
    "print(translate(sentence_en))\n",
    "print('='*30)\n",
    "print(beam_search(sentence_en, beam_width=3, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True, return_state=True))\n",
    "\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]\n",
    "decoder = keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "attention_layer = keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "y_proba = output_layer(attention_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[UNK] y [UNK] a [UNK] a [UNK]\n",
      "==============================\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "상위 첫 단어: [(-1.9274566, '[UNK]'), (-2.2856333, 'me'), (-3.6605964, 'quiero')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "지금까지 최상의 번역: [(-3.9298213, '[UNK] y'), (-3.9538238, '[UNK] [UNK]'), (-4.079806, 'me [UNK]')]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "지금까지 최상의 번역: [(-5.5314198, '[UNK] y [UNK]'), (-6.087785, 'me [UNK] y'), (-6.1089306, 'me [UNK] [UNK]')]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "지금까지 최상의 번역: [(-7.4136157, '[UNK] y [UNK] a'), (-7.6505456, 'me [UNK] y [UNK]'), (-8.026182, 'me [UNK] [UNK] a')]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "지금까지 최상의 번역: [(-9.002893, '[UNK] y [UNK] a [UNK]'), (-9.391769, 'me [UNK] y [UNK] a'), (-9.575634, 'me [UNK] [UNK] a [UNK]')]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "지금까지 최상의 번역: [(-10.744028, '[UNK] y [UNK] a [UNK] a'), (-10.990112, 'me [UNK] y [UNK] a [UNK]'), (-11.182905, '[UNK] y [UNK] a [UNK] la')]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "지금까지 최상의 번역: [(-12.444168, '[UNK] y [UNK] a [UNK] a [UNK]'), (-12.717768, 'me [UNK] y [UNK] a [UNK] endofseq'), (-13.042256, '[UNK] y [UNK] a [UNK] la [UNK]')]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "지금까지 최상의 번역: [(-12.717768, 'me [UNK] y [UNK] a [UNK] endofseq'), (-12.8087225, '[UNK] y [UNK] a [UNK] a [UNK] endofseq'), (-13.1991, '[UNK] y [UNK] a [UNK] la [UNK] endofseq')]\n",
      "me [UNK] y [UNK] a [UNK]\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"I like soccer and also going to the beach\"))\n",
    "print('='*30)\n",
    "print(beam_search(\"I like soccer and also going to the beach\", beam_width=3, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims%2 == 1:\n",
    "            max_dims += 1   # max_dims는 짝수여야 함\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims//2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        \n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "max_steps = 500\n",
    "vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)\n",
    "\n",
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    X = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    # Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z], use_causal_mask=True)    # tensorflow WARNING에 따라 변경\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation='softmax'))(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50  # 전체 훈련 세트에 있는 최대 길이\n",
    "embed_size = 128\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "pos_embed_layer = keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\n",
    "\n",
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    \n",
    "    skip = Z\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_max_len_dec = 50  # tf.shape(decoder_embeddings)[1]\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, None]\n",
    "causal_mask = tf.linalg.band_part(  # 행렬의 대각선 부분을 추출하거나 수정하는 함수\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask&decoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "\n",
    "    attn_layer = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "\n",
    "    Z = keras.layers.Dense(n_units, activation='relu')(Z)\n",
    "    Z = keras.layers.Dense(embed_size)(Z)\n",
    "    Z = keras.layers.LayerNormalization()(keras.layers.Add()([Z, skip]))\n",
    "\n",
    "y_proba = keras.layers.Dense(vocab_size, activation='softmax')(Z)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")  # 다른 많은 작업을 사용할 수 있습니다.\n",
    "result = classifier(\"The actors were very convincing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n",
       " {'label': 'NEGATIVE', 'score': 0.9811071157455444}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I am from India.\", \"I am from Iraq.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n",
       "         102,    0,    0,    0],\n",
       "       [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n",
       "        2003, 2214, 1012,  102]])>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer([(\"I like soccer.\", \"We all love soccer!\"),\n",
    "                       (\"Joe lived for a very long time.\", \"Joe is old.\")],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-2.1123827 ,  1.1786797 ,  1.4101014 ],\n",
       "       [-0.01478171,  1.0962448 , -0.9919954 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.01619699, 0.43523583, 0.5485672 ],\n",
       "       [0.22656046, 0.6881716 , 0.08526791]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "Y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1], dtype=int64)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x000002117FD7E170> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000002117FD7E170> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 31s 31s/step - loss: 1.5941 - accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.3027 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': 'January', '02': 'February', '03': 'March', '04': 'April', '05': 'May', '06': 'June', '07': 'July', '08': 'August', '09': 'September', '10': 'October', '11': 'November', '12': 'December'}\n",
      "April 04, 8436 3287182 8436-04-04 3287182\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, keras, tensorflow as tf\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "MONTHS_DICT = {f'{i:>02}': v for i, v in enumerate(MONTHS, 1)}\n",
    "print(MONTHS_DICT)\n",
    "\n",
    "start_date = np.datetime64('1000-01-01')\n",
    "end_date = np.datetime64('9999-12-31')\n",
    "y = np.arange(start_date, end_date + np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "np.random.shuffle(y)\n",
    "y = list(map(str, y))\n",
    "X = list(map(lambda x: MONTHS_DICT[x[5:7]] + ' ' + x[-2:] + ', ' + x[:4], y))\n",
    "print(X[0], len(X), y[0], len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,0123456789ADFJMNOSabceghilmnoprstuvy\n"
     ]
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "# MONTHS를 join 후 set으로 중복 문자 제거한 후 숫자와 쉼표, 공백을 포함하여 입력값 원소를 정렬하여 문자열로 생성\n",
    "print(INPUT_CHARS)\n",
    "OUTPUT_CHARS = \"0123456789-\"    # 출력값 원소를 문자열로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 31, 32, 26, 27, 0, 2, 6, 1, 0, 10, 6, 5, 8]\n",
      "[8, 4, 3, 6, 10, 0, 4, 10, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    \"\"\"date_str에서 INPUT_CHARS 인덱스로 ID를 생성하는 함수\"\"\"\n",
    "    return [chars.index(c) for c in date_str]\n",
    "print(date_str_to_ids(X[0], INPUT_CHARS))\n",
    "print(date_str_to_ids(y[0], OUTPUT_CHARS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # 0을 패딩 토큰 ID로 사용\n",
    "\n",
    "X = list(map(prepare_date_strs, X))\n",
    "\n",
    "prepare_date_strs = partial(prepare_date_strs, chars=OUTPUT_CHARS)\n",
    "y = list(map(prepare_date_strs, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
